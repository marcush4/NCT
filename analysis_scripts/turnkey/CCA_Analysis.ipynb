{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as glob\n",
    "import os\n",
    "import scipy\n",
    "import sys; sys.path.append('/home/marcush/projects/neural_control/analysis_scripts/turnkey')\n",
    "\n",
    "from dca.cov_util import form_lag_matrix, calc_cross_cov_mats_from_data\n",
    "from config import PATH_DICT; sys.path.append(PATH_DICT['repo'])\n",
    "from region_select import *\n",
    "from loaders import *\n",
    "from utils import calc_loadings\n",
    "from collections import defaultdict\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# New\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.model_selection import KFold\n",
    "from dca.cov_util import form_lag_matrix, calc_cross_cov_mats_from_data\n",
    "import glob\n",
    "import pdb\n",
    "from statsmodels.tsa import stattools\n",
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load consolidated decoding dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_glom_path = '/clusterfs/NSDS_data/FCCA/postprocessed/tsao_decode_df.pkl'\n",
    "with open(decoding_glom_path, 'rb') as f:\n",
    "    dat = pickle.load(f) \n",
    "\n",
    "df_decode = pd.DataFrame(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_splits = np.unique(df_decode['fold_idx'].values)\n",
    "sessions = np.unique(df_decode['data_file'])\n",
    "comm = MPI.COMM_WORLD\n",
    "sessions = np.array_split(sessions, comm.size)[comm.rank]\n",
    "regions = np.unique(df_decode['loader_args'].apply(lambda x: x.get('region')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Loading Data...\n",
      "Done Loading Data\n",
      "Begin getting spike times...\n",
      "Done getting spike times\n",
      "Begin filtering spike times into spike rates...\n",
      "FILTERING SPIKE RATES!\n",
      "Done filtering spike times into spike rates\n",
      "Begin Loading Data...\n",
      "Done Loading Data\n",
      "Begin getting spike times...\n",
      "Done getting spike times\n",
      "Begin filtering spike times into spike rates...\n",
      "FILTERING SPIKE RATES!\n",
      "Done filtering spike times into spike rates\n",
      "Begin Loading Data...\n",
      "Done Loading Data\n",
      "Begin getting spike times...\n",
      "Done getting spike times\n",
      "Begin filtering spike times into spike rates...\n",
      "FILTERING SPIKE RATES!\n",
      "Done filtering spike times into spike rates\n",
      "Begin Loading Data...\n",
      "Done Loading Data\n",
      "Begin getting spike times...\n",
      "Done getting spike times\n",
      "Begin filtering spike times into spike rates...\n",
      "FILTERING SPIKE RATES!\n",
      "Done filtering spike times into spike rates\n"
     ]
    }
   ],
   "source": [
    "all_spikes = {}\n",
    "for session in sessions:\n",
    "    all_spikes[session] = {}\n",
    "    for region in regions: \n",
    "    \n",
    "        df_ = apply_df_filters(df_decode, **{'loader_args':{'region': region}})\n",
    "        dat = load_tsao(**dict(df_['full_arg_tuple'][0]))            \n",
    "        all_spikes[session][region] = dat['spike_rates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data and perform CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcush/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/sklearn/cross_decomposition/_pls.py:113: ConvergenceWarning: Maximum number of iterations reached\n",
      "  warnings.warn(\"Maximum number of iterations reached\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcush/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/sklearn/cross_decomposition/_pls.py:113: ConvergenceWarning: Maximum number of iterations reached\n",
      "  warnings.warn(\"Maximum number of iterations reached\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     y \u001b[38;5;241m=\u001b[39m form_lag_matrix(y, window)\n\u001b[1;32m     39\u001b[0m ccamodel \u001b[38;5;241m=\u001b[39m CCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(max_cca_dim_check, \u001b[38;5;28mmin\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])))                \n\u001b[0;32m---> 40\u001b[0m \u001b[43mccamodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m X_c, Y_c \u001b[38;5;241m=\u001b[39m ccamodel\u001b[38;5;241m.\u001b[39mtransform(x, y)\n\u001b[1;32m     42\u001b[0m canonical_correlations \u001b[38;5;241m=\u001b[39m [scipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mpearsonr(X_c[:, i], Y_c[:, i])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_cca_dim_check)]\n",
      "File \u001b[0;32m~/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/sklearn/cross_decomposition/_pls.py:294\u001b[0m, in \u001b[0;36m_PLS.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    287\u001b[0m Yk[:, Yk_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     (\n\u001b[1;32m    291\u001b[0m         x_weights,\n\u001b[1;32m    292\u001b[0m         y_weights,\n\u001b[1;32m    293\u001b[0m         n_iter_,\n\u001b[0;32m--> 294\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_first_singular_vectors_power_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mYk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_y_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_y_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY residual is constant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/sklearn/cross_decomposition/_pls.py:85\u001b[0m, in \u001b[0;36m_get_first_singular_vectors_power_method\u001b[0;34m(X, Y, mode, max_iter, tol, norm_y_weights)\u001b[0m\n\u001b[1;32m     76\u001b[0m x_weights_old \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# init to big value for first convergence check\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Precompute pseudo inverse matrices\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Basically: X_pinv = (X.T X)^-1 X.T\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# B) will be unstable if n_features > n_samples or n_targets >\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# n_samples\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     X_pinv, Y_pinv \u001b[38;5;241m=\u001b[39m \u001b[43m_pinv2_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, _pinv2_old(Y)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/sklearn/cross_decomposition/_pls.py:47\u001b[0m, in \u001b[0;36m_pinv2_old\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pinv2_old\u001b[39m(a):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Used previous scipy pinv2 that was updated in:\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# https://github.com/scipy/scipy/pull/10067\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# We can not set `cond` or `rcond` for pinv2 in scipy >= 1.3 to keep the\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# same behavior of pinv2 for scipy < 1.3, because the condition used to\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# determine the rank is dependent on the output of svd.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     t \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     50\u001b[0m     factor \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e6\u001b[39m}\n",
      "File \u001b[0;32m~/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/scipy/linalg/_decomp_svd.py:141\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    137\u001b[0m lwork \u001b[38;5;241m=\u001b[39m _compute_lwork(gesXd_lwork, a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    138\u001b[0m                        compute_uv\u001b[38;5;241m=\u001b[39mcompute_uv, full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m u, s, v, info \u001b[38;5;241m=\u001b[39m \u001b[43mgesXd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_uv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RELOAD = True\n",
    "max_cca_dim_check = 50\n",
    "\n",
    "tmp_path = PATH_DICT['tmp']\n",
    "save_path_cca_corrs = f\"{tmp_path}/CCA_Analysis_Tsao.pkl\" \n",
    "\n",
    "if RELOAD:\n",
    "\n",
    "    lags = np.array([0])\n",
    "    windows = np.array([1])\n",
    "    reg0 = regions[0]\n",
    "    reg1 = regions[1]\n",
    "    results = []\n",
    "\n",
    "    for session in sessions:\n",
    "\n",
    "        X = all_spikes[session]['ML']\n",
    "        Y = all_spikes[session]['AM']\n",
    "        \n",
    "        X = X.reshape(-1, X.shape[-1])\n",
    "        Y = Y.reshape(-1, Y.shape[-1])\n",
    "\n",
    "        for k, lag in enumerate(lags):\n",
    "            for w, window in enumerate(windows):\n",
    "                for fold_idx, (train_idxs, test_idxs) in enumerate(KFold(n_splits=len(fold_splits)).split(X)):\n",
    "                \n",
    "                    x = X[train_idxs]\n",
    "                    y = Y[train_idxs]\n",
    "\n",
    "                    # Apply window and lag relative to each other\n",
    "                    if lag != 0:\n",
    "                        x = x[:-lag, :]\n",
    "                        y = x[lag:, :]\n",
    "\n",
    "                    if window > 1:\n",
    "                        x = form_lag_matrix(x, window)\n",
    "                        y = form_lag_matrix(y, window)\n",
    "\n",
    "                    ccamodel = CCA(n_components=min(max_cca_dim_check, min(x.shape[-1], y.shape[-1])))                \n",
    "                    ccamodel.fit(x, y)\n",
    "                    X_c, Y_c = ccamodel.transform(x, y)\n",
    "                    canonical_correlations = [scipy.stats.pearsonr(X_c[:, i], Y_c[:, i])[0] for i in range(max_cca_dim_check)]\n",
    "\n",
    "                    r = {\n",
    "                    'dfile': session,\n",
    "                    'lag': lag,\n",
    "                    'win': window,\n",
    "                    'fold_idx': fold_idx,\n",
    "                    'ccamodel': ccamodel,\n",
    "                    'canonical_correlations': canonical_correlations\n",
    "                    \n",
    "                    }\n",
    "                    results.append(r)\n",
    "                    print(f\"Done with fold {fold_idx+1}\")\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    with open(save_path_cca_corrs, 'wb') as f:\n",
    "        pickle.dump(df_results, f)\n",
    "else:\n",
    "\n",
    "    with open(save_path_cca_corrs, 'rb') as f:\n",
    "        df_results = pickle.load(f)\n",
    "    print(\"Loading previous CCA fit to split data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find saturating CCA dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p <= q\n",
    "def CC_AIC(cc_coefs, N, p, q):\n",
    "    # Sort in descending order\n",
    "    cc_coefs = np.sort(cc_coefs)[::-1]\n",
    "\n",
    "    # Calculate the vector Ak\n",
    "    Ak = np.array([-N * np.sum(np.log(1 - np.power(cc_coefs[k + 1:], 2))) -2 * (p - k) * (q - k) for k in range(cc_coefs.size - 1)])\n",
    "    return Ak\n",
    "\n",
    "def CC_BIC(cc_coefs, N, p, q):\n",
    "    # Sort in descending order\n",
    "    cc_coefs = np.sort(cc_coefs)[::-1]\n",
    "\n",
    "    # Calculate the vector Ak\n",
    "    Ak = np.array([-N * np.sum(np.log(1 - np.power(cc_coefs[k + 1:], 2))) -np.log(N) * (p - k) * (q - k) for k in range(cc_coefs.size - 1)])\n",
    "    return Ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session in sessions:\n",
    "    \n",
    "    df_results_sess = apply_df_filters(df_results, **{'session':session})\n",
    "\n",
    "    canonical_correlations = np.array(df_results_sess['canonical_correlations'].to_list())\n",
    "    \n",
    "    ### FIGURE 1\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    medianprops = dict(linewidth=0)\n",
    "    bplot = ax.boxplot(np.reshape(canonical_correlations, (-1, max_cca_dim_check)), patch_artist=True, medianprops=medianprops, notch=True)\n",
    "    nTicks = 10\n",
    "    ax.set_xticks(np.arange(1, max_cca_dim_check, nTicks))\n",
    "    ax.set_xticklabels(np.arange(1, max_cca_dim_check, nTicks))\n",
    "    ax.set_xlim([0, max_cca_dim_check])\n",
    "    ax.set_ylabel('Canonical Correlation Coefficient')\n",
    "    ax.set_xlabel('Dimension')\n",
    "    ax.set_title(session)\n",
    "    \n",
    "    \n",
    "    #### FIGURE 2\n",
    "    nSplits = len(fold_splits)\n",
    "    cc_dim = np.zeros((nSplits, 2))\n",
    "\n",
    "    for split in fold_splits: \n",
    "        train_idxs, test_idxs = list(KFold(n_splits=nSplits).split(X))[split]\n",
    "\n",
    "        x = X[train_idxs]\n",
    "        y = Y[train_idxs]\n",
    "\n",
    "        p = min(x.shape[1], y.shape[1])\n",
    "        q = max(x.shape[1], y.shape[1])\n",
    "\n",
    "        Ak = CC_AIC(canonical_correlations[split, :], x.shape[0], p, q)\n",
    "        cc_dim[split, 0] = np.argmin(Ak)\n",
    "        Ak = CC_BIC(canonical_correlations[split, :], x.shape[0], p, q)    \n",
    "        cc_dim[split, 1] = np.argmin(Ak)\n",
    "                \n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.boxplot(cc_dim + 1)\n",
    "    ax.set_xticklabels(['AIC', 'BIC'])\n",
    "    ax.set_ylabel('Dimension')\n",
    "    ax.set_title(session)\n",
    "    \n",
    "    \n",
    "    print(f\"Optimal AIC Dim for session {session}: {np.median(cc_dim[:,0])}\")\n",
    "    print(f\"Optimal BIC Dim for session {session}: {np.median(cc_dim[:,1])}\")\n",
    "    print(f\"Optimal CCA Dim for session {session}: {np.mean([np.median(cc_dim[:,0]), np.median(cc_dim[:,1])])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and Save CCA Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for session in sessions:\n",
    "        \n",
    "    X = all_spikes[session]['ML']\n",
    "    Y = all_spikes[session]['AM']\n",
    "    \n",
    "    X = X.reshape(-1, X.shape[-1])\n",
    "    Y = Y.reshape(-1, Y.shape[-1])\n",
    "\n",
    "    manual_CCA_dim = 21 \n",
    "    ccamodel = CCA(n_components=manual_CCA_dim)\n",
    "    ccamodel.fit(X, Y)\n",
    "        \n",
    "    cca_save_path = f'/clusterfs/NSDS_data/FCCA/postprocessed/CCA_structs/CCA_{session}_{manual_CCA_dim}_dims.pkl'\n",
    "\n",
    "    with open(cca_save_path, 'wb') as file:\n",
    "        pickle.dump(ccamodel, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
