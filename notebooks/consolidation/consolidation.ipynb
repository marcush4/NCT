{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from deepdiff import DeepDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/akumar/nse/neural_control')\n",
    "from loaders import load_sabes\n",
    "from utils import apply_df_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "peanut_decoding_files = glob.glob('/mnt/Secondary/data/peanut_decoding01/peanut_decoding01_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for file in peanut_decoding_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "peanut_decoding_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/peanut_decoding_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(peanut_decoding_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_decoding_files = glob.glob('/mnt/Secondary/data/sabes_decoding01/sabes_decoding01_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for file in sabes_decoding_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_decoding_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/sabes_decoding_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(sabes_decoding_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conslidating 4/27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indyparametric_files = glob.glob('/mnt/Secondary/data/indy_dimreduc_parametric/indy_dimreduc_parametric_*.dat')\n",
    "argfiles = glob.glob('/mnt/Secondary/data/indy_dimreduc_parametric/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            result_[k] = v\n",
    "    \n",
    "    result_list.extend(result)\n",
    "dimreduc_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indy_parametric_dimreduc_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(dimreduc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimreduc_files = glob.glob('/mnt/Secondary/data/indy_trialized_dimreduc/indy_trialized_dimreduc_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for file in dimreduc_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    result_list.extend(result)\n",
    "dimreduc_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indy_trialized_dimreduc_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(dimreduc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_files = glob.glob('/mnt/Secondary/data/peanut_decoding_fcca/peanut_decoding_fcca_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for file in decoding_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    result_list.extend(result)\n",
    "decoding_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/peanut_fcca_decoding_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(decoding_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_files = glob.glob('/mnt/Secondary/data/indydimreducparam_decoding/indydimreducparam_decoding_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "\n",
    "\n",
    "result_list = []\n",
    "for file in decoding_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    result_list.extend(result)\n",
    "decoding_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/indydimreducparam_decoding/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_args = []\n",
    "for argfile in argfiles:\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "    \n",
    "    # Track down the arg file associated with the dimreduc result\n",
    "    dimreducpath = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "    dimreducno = args['task_args']['dimreduc_file'].split('.dat')[0].split('_')[-1]\n",
    "    argfile = '%s/arg%s.dat' % (dimreducpath, dimreducno)    \n",
    "    \n",
    "    with open(argfile, 'rb') as f:\n",
    "        dimreduc_args = pickle.load(f)\n",
    "    \n",
    "    loader_args.append(dimreduc_args['loader_args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_loader_args = []\n",
    "for loader_arg in loader_args:\n",
    "    if not unique_loader_args:\n",
    "        unique_loader_args.append(loader_arg)\n",
    "    else:\n",
    "        diffs = []\n",
    "        for ula in unique_loader_args:\n",
    "            diffs.append(DeepDiff(ula, loader_arg))\n",
    "        \n",
    "        diffs = np.array([len(d) for d in diffs])\n",
    "        if np.all(diffs > 0):\n",
    "            unique_loader_args.append(loader_arg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separately agregarate and save for each loader arg\n",
    "for i, ula in enumerate(unique_loader_args):\n",
    "    results = []\n",
    "    for loader_arg, argfile in zip(loader_args, argfiles):\n",
    "        if len(DeepDiff(loader_arg, ula)) == 0:\n",
    "            # Open up the results file\n",
    "            with open(argfile, 'rb') as f:\n",
    "                args = pickle.load(f)\n",
    "\n",
    "            # Open up the dimreduc args save the args with the results\n",
    "            # Track down the arg file associated with the dimreduc result\n",
    "            dimreducpath = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "            dimreducno = args['task_args']['dimreduc_file'].split('.dat')[0].split('_')[-1]\n",
    "            argfile = '%s/arg%s.dat' % (dimreducpath, dimreducno)    \n",
    "            \n",
    "            with open(argfile, 'rb') as f:\n",
    "                dimreduc_args = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "            results_file = args['results_file']\n",
    "            with open(results_file, 'rb') as f:\n",
    "                result = pickle.load(f)\n",
    "                for result_ in result:\n",
    "                    for k, v in args.items():\n",
    "                        result_[k] = v\n",
    "                    for k, v in dimreduc_args.items():\n",
    "                        result_[k] = v\n",
    "\n",
    "            results.extend(result)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    with open('/home/akumar/nse/neural_control/data/indy_parametric_decoding/df%d.dat' % i, 'wb') as f:\n",
    "        f.write(pickle.dumps(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dim                                                               10\n",
       "fold_idx                                                           0\n",
       "train_idxs         [11706, 11707, 11708, 11709, 11710, 11711, 117...\n",
       "test_idxs          [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "dimreduc_method                                                  DCA\n",
       "dimreduc_args                                  {'T': 3, 'n_init': 5}\n",
       "coef               [[0.043348193288712804, 0.06172647904631565, -...\n",
       "score                                                      68.058941\n",
       "decoder                                                           lr\n",
       "decoder_args       {'trainlag': 4, 'testlag': 4, 'decoding_window...\n",
       "decoder_obj                         LinearRegression(normalize=True)\n",
       "r2                 [0.14425542097701782, 0.2055141988659695, 0.02...\n",
       "data_file             /mnt/Secondary/data/sabes/indy_20160630_01.mat\n",
       "loader                                                         sabes\n",
       "loader_args        {'bin_width': 25, 'filter_fn': 'window', 'filt...\n",
       "task_args          {'dim_vals': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1...\n",
       "data_path                                  /mnt/Secondary/data/sabes\n",
       "results_file       /mnt/Secondary/data/indy_dimreduc_parametric/i...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/indytrialized_decoding/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "not_done = []\n",
    "for argfile in argfiles:\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    # Open up the dimreduc args save the args with the results\n",
    "    # Track down the arg file associated with the dimreduc result\n",
    "    dimreducpath = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "    dimreducno = args['task_args']['dimreduc_file'].split('.dat')[0].split('_')[-1]\n",
    "    argfile = '%s/arg%s.dat' % (dimreducpath, dimreducno)    \n",
    "    \n",
    "    with open(argfile, 'rb') as f:\n",
    "        dimreduc_args = pickle.load(f)\n",
    "\n",
    "    results_file = args['results_file']\n",
    "    try:\n",
    "        with open(results_file, 'rb') as f:\n",
    "            result = pickle.load(f)\n",
    "            for result_ in result:\n",
    "                for k, v in args.items():\n",
    "                    result_[k] = v\n",
    "                for k, v in dimreduc_args.items():\n",
    "                    result_[k] = v\n",
    "    except FileNotFoundError:\n",
    "        not_done.append(results_file)\n",
    "    result_list.extend(result)\n",
    "decoding_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [int(n.split('.dat')[0].split('_')[-1]) for n in not_done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files that did not complete here are results of T being too long for trialized trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  189,  193,  197,  205,  209,  213,  217,  221,  229,  233,\n",
       "        237,  433,  437,  441,  449,  453,  457,  461,  465,  469,  637,\n",
       "        909, 1245, 1249, 1253, 1397, 1441, 1445, 1449, 1457, 1461, 1465,\n",
       "       1469, 1473, 1477])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indytrialized_decoding_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(decoding_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter loco UoI data files and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/mnt/Secondary/data/sabes'    \n",
    " \n",
    "# These are the data files that contain both M1 and S1 recordings.\n",
    "data_files = glob.glob('%s/loco*' % data_path)\n",
    "data_files = [data_files[0], data_files[5]]\n",
    "loader_args = {'bin_width':50, 'filter_fn':'none', 'filter_kwargs':{}, 'boxcox':0.5, 'spike_threshold':100, 'region':'S1'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.20s/it]\n"
     ]
    }
   ],
   "source": [
    "d1 = load_sabes(data_files[0], **loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.91s/it]\n"
     ]
    }
   ],
   "source": [
    "d2 = load_sabes(data_files[1], **loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loco_20170210_03.mat'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files[0].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/Secondary/data/sabes/preprocessed/%s' % data_files[0].split('/')[-1], 'wb') as f:\n",
    "    f.write(pickle.dumps(d1))\n",
    "    f.write(pickle.dumps((data_files[0], loader_args)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/Secondary/data/sabes/preprocessed/%s' % data_files[1].split('/')[-1], 'wb') as f:\n",
    "    f.write(pickle.dumps(d2))\n",
    "    f.write(pickle.dumps((data_files[1], loader_args)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating CV Dimreudc with longer T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/cv_dimreduc2/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            result_[k] = v\n",
    "    \n",
    "    result_list.extend(result)\n",
    "dimreduc_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/cv_dimreduc_df2.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T': 10, 'n_init': 5}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimreduc_df.iloc[0]['dimreduc_args']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV_dimreduc aross both subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/cv_dimreduc_both/arg*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = []\n",
    "for f_ in argfiles:\n",
    "    with open(f_, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            result_[k] = v\n",
    "    \n",
    "    rl.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/cv_dimreduc_both.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(rl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indy signfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/sabes_signfix_dw5/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(argfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    # Need to grab some information from the dimreduc arg file\n",
    "    dimreduc_path = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "    dimreduc_no = args['task_args']['dimreduc_file'].split('_')[-1].split('.dat')[0]\n",
    "    dimreduc_argfile = '%s/arg%s.dat' % (dimreduc_path, dimreduc_no)\n",
    "\n",
    "    with open(dimreduc_argfile, 'rb') as f:\n",
    "        dr_args = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in dr_args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8400"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dim', 'fold_idx', 'train_idxs', 'test_idxs', 'dimreduc_method', 'dimreduc_args', 'coef', 'score', 'decoder', 'decoder_args', 'decoder_obj', 'r2', 'task_args', 'data_file', 'data_path', 'loader', 'loader_args', 'results_file'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/sabes_decoding_sf.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import apply_df_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = np.unique(rl['data_file'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/mnt/Secondary/data/sabes/indy_20160426_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160622_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160624_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160627_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160630_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160915_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160921_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160930_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160930_05.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161005_06.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161006_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161007_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161011_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161013_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161014_04.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161017_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161024_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161025_04.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161026_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161027_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161206_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161207_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161212_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161220_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20170123_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20170124_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20170127_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20170131_02.mat'], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33600, 18)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apply_df_filters(rl, data_file=data_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 18)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30 * 5 * 4 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20400 * 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new sabes decoding df that is just PCA, LQGCA signfixed, DCA, and KCA fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/sabes_decoding_df.dat', 'rb') as f:\n",
    "    sabes_decoding_df = pickle.load(f)\n",
    "with open('/home/akumar/nse/neural_control/data/sabes_decoding_sf.dat', 'rb') as f:\n",
    "    sabes_decoding_sf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_decoding_sf = pd.DataFrame(sabes_decoding_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import apply_df_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "KCA_dimreduc_args =  [{'T':3, 'causal_weights':(1, 0), 'n_init':5}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dim', 'fold_idx', 'train_idxs', 'test_idxs', 'dimreduc_method',\n",
       "       'dimreduc_args', 'coef', 'score', 'bin_width', 'filter_fn',\n",
       "       'filter_kwargs', 'boxcox', 'spike_threshold', 'dim_vals', 'n_folds',\n",
       "       'data_file', 'decoder', 'decoder_args', 'decoder_obj', 'r2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sabes_decoding_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dim', 'fold_idx', 'train_idxs', 'test_idxs', 'dimreduc_method',\n",
       "       'dimreduc_args', 'coef', 'score', 'decoder', 'decoder_args',\n",
       "       'decoder_obj', 'r2', 'task_args', 'data_file', 'data_path', 'loader',\n",
       "       'loader_args', 'results_file'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sabes_decoding_sf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bin_width': 50,\n",
       " 'filter_fn': 'none',\n",
       " 'filter_kwargs': {},\n",
       " 'boxcox': 0.5,\n",
       " 'spike_threshold': 100,\n",
       " 'region': 'M1'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sabes_decoding_sf.iloc[0]['loader_args']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating FCCA with proper normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/indy_norm_decoding/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    # Need to grab some information from the dimreduc arg file\n",
    "    dimreduc_path = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "    dimreduc_no = args['task_args']['dimreduc_file'].split('_')[-1].split('.dat')[0]\n",
    "    dimreduc_argfile = '%s/arg%s.dat' % (dimreduc_path, dimreduc_no)\n",
    "\n",
    "    with open(dimreduc_argfile, 'rb') as f:\n",
    "        dr_args = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in dr_args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indy_norm_decoding.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the rest of sabes dimreduc with indy norm for easue of use\n",
    "with open('/home/akumar/nse/neural_control/data/indy_norm_decoding.dat','rb') as f:\n",
    "    results_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ease of use, use data file names that do not involve the directory path\n",
    "for r in results_list:\n",
    "    r['data_file'] = r['data_file'].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_new = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/sabes_decoding_df.dat', 'rb') as f:\n",
    "    sabes_old = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfiles1 = np.unique(pd.DataFrame(results_list)['data_file'].values)\n",
    "dfiles2 = np.unique(sabes_old['data_file'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_old_dca = apply_df_filters(sabes_old, dimreduc_method='DCA')\n",
    "sabes_old_pca = apply_df_filters(sabes_old, dimreduc_method='PCA')\n",
    "sabes_old_kca = apply_df_filters(sabes_old, dimreduc_method='KCA', dimreduc_args={'T':3, 'causal_weights':(1, 0), 'n_init':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_new_fca = apply_df_filters(sabes_new, dimreduc_args={'T':3, 'loss_type':'trace', 'n_init':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_consolidated = pd.concat([sabes_old_dca, sabes_old_pca, sabes_old_kca, sabes_new], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indy_decoding_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(sabes_consolidated.to_dict('records')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a similar consolidation for peanut - note we rerun with the final FCCA version here\n",
    "argfiles = glob.glob('/mnt/Secondary/data/peanut_decoding_norm/arg*.dat')\n",
    "\n",
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            if type(v) == dict:\n",
    "                for k_, v_ in v.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    # Need to grab some information from the dimreduc arg file\n",
    "    dimreduc_path = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "    dimreduc_no = args['task_args']['dimreduc_file'].split('_')[-1].split('.dat')[0]\n",
    "    dimreduc_argfile = '%s/arg%s.dat' % (dimreduc_path, dimreduc_no)\n",
    "\n",
    "    with open(dimreduc_argfile, 'rb') as f:\n",
    "        dr_args = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in dr_args.items():\n",
    "            if type(v) == dict:\n",
    "                for k_, v_ in v.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peanut_new = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/peanut_decoding_df.dat', 'rb') as f:\n",
    "    peanut_old = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "peanut_old = pd.DataFrame(peanut_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "peanut_old_dca = apply_df_filters(peanut_old, dimreduc_method='DCA')\n",
    "peanut_old_pca = apply_df_filters(peanut_old, dimreduc_method='PCA')\n",
    "peanut_old_kca = apply_df_filters(peanut_old, dimreduc_method='KCA', dimreduc_args={'T':3, 'causal_weights':(1, 0), 'n_init':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "peanut_new_fca = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "peanut_df = pd.concat([peanut_old_dca, peanut_old_pca, peanut_old_kca, peanut_new], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = apply_df_filters(peanut_old_pca, epoch=2, dimreduc_method='PCA', fold_idx=0, dim=2, \n",
    "                       decoder_args={'trainlag': 3, 'testlag': 3, 'decoding_window': 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 27)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/peanut_decoding_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(peanut_df.to_dict('records')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating No CV Indy Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a similar consolidation for peanut\n",
    "argfiles = glob.glob('/mnt/Secondary/data/indy_dimreduc_nocv/arg*.dat')\n",
    "\n",
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            if type(v) == dict:\n",
    "                for k_, v_ in v.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indy_dimreduc_nocv.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(result_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6/28 - modification to reverse time normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/indy_norm_decoding2b/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dimreduc_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32547/3406629307.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Need to grab some information from the dimreduc arg file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdimreduc_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task_args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dimreduc_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mdimreduc_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task_args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dimreduc_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.dat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdimreduc_argfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s/arg%s.dat'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdimreduc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimreduc_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dimreduc_file'"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    # Need to grab some information from the dimreduc arg file\n",
    "    dimreduc_path = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "    dimreduc_no = args['task_args']['dimreduc_file'].split('_')[-1].split('.dat')[0]\n",
    "    dimreduc_argfile = '%s/arg%s.dat' % (dimreduc_path, dimreduc_no)\n",
    "\n",
    "    with open(dimreduc_argfile, 'rb') as f:\n",
    "        dr_args = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in dr_args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ease of use, use data file names that do not involve the directory path\n",
    "for r in result_list:\n",
    "    r['data_file'] = r['data_file'].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/sabes_decoding_df.dat', 'rb') as f:\n",
    "    sabes_old = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_new = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_new_fca = apply_df_filters(sabes_new, dimreduc_args={'T':3, 'loss_type':'trace', 'n_init':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_old_dca = apply_df_filters(sabes_old, dimreduc_method='DCA')\n",
    "sabes_old_pca = apply_df_filters(sabes_old, dimreduc_method='PCA')\n",
    "sabes_old_kca = apply_df_filters(sabes_old, dimreduc_method='KCA', dimreduc_args={'T':3, 'causal_weights':(1, 0), 'n_init':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_consolidated = pd.concat([sabes_old_dca, sabes_old_pca, sabes_old_kca, sabes_new], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indy_decoding_df2.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(sabes_consolidated.to_dict('records')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating marginal fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/indy_decoding_marginal/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    # Need to grab some information from the dimreduc arg file\n",
    "    dimreduc_path = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "    dimreduc_no = args['task_args']['dimreduc_file'].split('_')[-1].split('.dat')[0]\n",
    "    dimreduc_argfile = '%s/arg%s.dat' % (dimreduc_path, dimreduc_no)\n",
    "\n",
    "    with open(dimreduc_argfile, 'rb') as f:\n",
    "        dr_args = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in dr_args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ease of use, use data file names that do not involve the directory path\n",
    "for r in result_list:\n",
    "    r['data_file'] = r['data_file'].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indy_decoding_marginal.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/sabes_marginal_dimreduc_nocv/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ease of use, use data file names that do not involve the directory path\n",
    "for r in result_list:\n",
    "    r['data_file'] = r['data_file'].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indy_dimreduc_marginal_nocv.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(result_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV dimreduc with proper FCCA normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/cv_dimreduc_norm/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/cv_dimreduc_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(result_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loco dimreduc/decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c52494c424e88c3f855a8aeb34b231af4706f7aa247f66fb47c890a5ab8814ab"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('dyn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
