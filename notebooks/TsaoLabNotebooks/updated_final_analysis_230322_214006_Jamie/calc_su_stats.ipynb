{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import time\n",
    "import torch\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/marcush/projects/neural_control/')\n",
    "sys.path.append('/home/marcush/projects/neural_control/analysis_scripts/')\n",
    "sys.path.append('/home/marcush/projects/github_repos')\n",
    "from utils import apply_df_filters, calc_loadings, calc_cascaded_loadings\n",
    "from loaders import load_sabes, load_peanut, load_cv, load_tsao\n",
    "from decoders import lr_decoder, lr_encoder, logreg_decoder, categorical_reg\n",
    "from subspaces import SubspaceIdentification, IteratedStableEstimator, estimate_autocorrelation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from dca.cov_util import calc_cross_cov_mats_from_data, calc_pi_from_cross_cov_mats\n",
    "#from dca_research.kca import calc_mmse_from_cross_cov_mats\n",
    "#from dca_research.lqg import build_loss as build_lqg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace this with a loaders call!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataframe(s)\n",
    "dataframe_path = '/home/marcush/Data/TsaoLabData/neural_control_output_new/decoding_deg_230322_214006_Jamie/decoding_deg_230322_214006_Jamie_glom.pickle'\n",
    "savePath = os.path.dirname(dataframe_path)\n",
    "with open(dataframe_path, 'rb') as f:\n",
    "    rl = pickle.load(f)\n",
    "tsao_df = pd.DataFrame(rl)\n",
    "\n",
    "\n",
    "data_files = [tsao_df['data_file'][0]]\n",
    "# Loop through the rest of this from here\n",
    "\n",
    "# Load the spike rates\n",
    "def make_hashable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return tuple(sorted((key, make_hashable(value)) for key, value in obj.items()))\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return tuple(make_hashable(item) for item in obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "unique_hashes = set(make_hashable(d) for d in tsao_df['full_arg_tuple'])\n",
    "unique_dicts = [dict(u) for u in unique_hashes]\n",
    "preload_dict_path = tsao_df['data_path'][0] + \"/preloaded/preloadDict.pickle\"\n",
    "\n",
    "with open(preload_dict_path, 'rb') as file:\n",
    "    preloadDict = pickle.load(file)\n",
    "\n",
    "\n",
    "for arg_dict in unique_dicts:\n",
    "    arg_tuple = tuple(sorted(arg_dict.items()))\n",
    "\n",
    "\n",
    "    for args in preloadDict.keys():\n",
    "\n",
    "        if args == arg_tuple:\n",
    "\n",
    "            preloadID = preloadDict[arg_tuple]\n",
    "            loaded_data_path = os.path.dirname(preload_dict_path) + f\"/preloaded_data_{preloadID}.pickle\"\n",
    "            \n",
    "            if arg_dict['region'] == 'AM':\n",
    "                with open(loaded_data_path, 'rb') as file:\n",
    "                    AM_loaded_data = pickle.load(file)\n",
    "\n",
    "            elif arg_dict['region'] == 'ML':\n",
    "                with open(loaded_data_path, 'rb') as file:\n",
    "                    ML_loaded_data = pickle.load(file)\n",
    "\n",
    "\n",
    "AM_sp_rates = AM_loaded_data['spike_rates']\n",
    "ML_sp_rates = ML_loaded_data['spike_rates']\n",
    "\n",
    "AM_spike_mat = np.sum(AM_sp_rates, 1).squeeze()\n",
    "ML_spike_mat = np.sum(ML_sp_rates, 1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"ML\"\n",
    "\n",
    "\n",
    "\n",
    "if region == 'AM':\n",
    "    X = AM_spike_mat\n",
    "    Z = OneHotEncoder(sparse_output=False).fit_transform(AM_loaded_data['StimIDs'].reshape(-1, 1))\n",
    "elif region == 'ML':\n",
    "    X = ML_spike_mat\n",
    "    Z = OneHotEncoder(sparse_output=False).fit_transform(ML_loaded_data['StimIDs'].reshape(-1, 1))\n",
    "# Make targets one-hot since categorical\n",
    "\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits=n_splits, shuffle=False)\n",
    "decoder_params = tsao_df['decoder_args'][0]\n",
    "\n",
    "tsao_results = []\n",
    "\n",
    "# Average results across folds\n",
    "decoding_weights = []\n",
    "encoding_weights = []\n",
    "\n",
    "su_decoding_weights = []\n",
    "su_encoding_weights = []\n",
    "\n",
    "su_decoding_r2 = []\n",
    "su_encoding_r2 = []\n",
    "\n",
    "# Single unit statistics\n",
    "su_var = np.zeros((n_splits, X.shape[-1]))\n",
    "su_act = np.zeros((n_splits, X.shape[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [03:40, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m region \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mML\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     18\u001b[0m     ccm_xtrain \u001b[38;5;241m=\u001b[39m list_of_arrays \u001b[38;5;241m=\u001b[39m [ML_sp_rates[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ML_sp_rates\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])] \u001b[38;5;66;03m# just pass this as list of trials\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     ccm \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_cross_cov_mats_from_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mccm_xtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mML_sp_rates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m ccm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ccm)\n\u001b[1;32m     23\u001b[0m _, decodingregressor \u001b[38;5;241m=\u001b[39m logreg_decoder(xtest, xtrain, ztest, ztrain)\n",
      "File \u001b[0;32m~/projects/github_repos/DynamicalComponentsAnalysis/src/dca/cov_util.py:243\u001b[0m, in \u001b[0;36mcalc_cross_cov_mats_from_data\u001b[0;34m(X, T, mean, chunks, stride, rng, regularization, reg_ops, stride_tricks, logger, method)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Xi \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[1;32m    241\u001b[0m     X_with_lags \u001b[38;5;241m=\u001b[39m form_lag_matrix(Xi, T, stride\u001b[38;5;241m=\u001b[39mstride, stride_tricks\u001b[38;5;241m=\u001b[39mstride_tricks,\n\u001b[1;32m    242\u001b[0m                                   rng\u001b[38;5;241m=\u001b[39mrng)\n\u001b[0;32m--> 243\u001b[0m     cov_est \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_with_lags\u001b[38;5;241m.\u001b[39mT, X_with_lags)\n\u001b[1;32m    244\u001b[0m     n_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_with_lags)\n\u001b[1;32m    245\u001b[0m cov_est \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (n_samples \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        # Use time resolved responses to determine unit covariances, and use as much time as possible to estimate this.\n",
    "        if region == 'AM':\n",
    "            ccm_xtrain = list_of_arrays = [AM_sp_rates[i] for i in range(AM_sp_rates.shape[0])] # just pass this as list of trials\n",
    "            ccm = calc_cross_cov_mats_from_data(ccm_xtrain, T=AM_sp_rates.shape[1]-1)\n",
    "        elif region == 'ML':\n",
    "            ccm_xtrain = list_of_arrays = [ML_sp_rates[i] for i in range(ML_sp_rates.shape[0])] # just pass this as list of trials\n",
    "            ccm = calc_cross_cov_mats_from_data(ccm_xtrain, T=ML_sp_rates.shape[1]-1)\n",
    "        \n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, decodingregressor = logreg_decoder(xtest, xtrain, ztest, ztrain)\n",
    "        _, encodingregressor = categorical_reg(xtest, xtrain, ztest, ztrain) # Use categorical regression for the encoder model\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        \n",
    "        su_dw = []     # Single Unit Decoding Weights\n",
    "        su_ew = []     # Single Unit Encoding Weights\n",
    "        su_dec_r2 = [] # (McFadden's) R^2 value of predicting stimID from neural activity\n",
    "        su_enc_r2 = [] #              R^2 value of predicting neural activity from stimID\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_decoding, decReg = logreg_decoder(xtest, xtrain, ztest, ztrain)\n",
    "            su_dw.append(decReg.coef_)\n",
    "            su_dec_r2.append(r2_decoding)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding, encReg = categorical_reg(xtest, xtrain, ztest, ztrain)\n",
    "            su_ew.append(encReg.coef_)        \n",
    "            su_enc_r2.append(r2_encoding)\n",
    "\n",
    "\n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_decoding_r2.append(np.array(su_dec_r2))\n",
    "        su_encoding_r2.append(np.array(su_enc_r2))\n",
    "\n",
    "        \n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            \n",
    "            ccm_j = ccm[:, neu_idx, neu_idx].numpy()\n",
    "            ccm_j /= ccm_j[0]\n",
    "\n",
    "            thr = 1e-1\n",
    "            acov_crossing = np.where(ccm_j < thr)\n",
    "            if len(acov_crossing[0]) > 0:\n",
    "                su_act[fold_idx, neu_idx] = np.where(ccm_j < thr)[0][0]\n",
    "            else:\n",
    "                su_act[fold_idx, neu_idx] = len(ccm)\n",
    "\n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "        print(f\"Done with {fold_idx+1} fold\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average results across folds and save\n",
    "decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "\n",
    "su_decoding_r2 = np.mean(np.array(su_decoding_r2), axis=0)\n",
    "su_encoding_r2 = np.mean(np.array(su_encoding_r2), axis=0)\n",
    "\n",
    "su_var = np.mean(su_var, axis=0)\n",
    "su_act = np.mean(su_act, axis=0)\n",
    "\n",
    "result = {}\n",
    "for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_decoding_r2'\n",
    "                 , 'su_encoding_r2', 'su_var', 'su_act', 'decoder_params'):\n",
    "    result[variable] = eval(variable)\n",
    "\n",
    "tsao_results.append(result)\n",
    "\n",
    "with open(f'{savePath}/tsao_su_calcs_{region}.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(tsao_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"AM\"\n",
    "\n",
    "\n",
    "if region == 'AM':\n",
    "    X = AM_spike_mat\n",
    "    Z = OneHotEncoder(sparse_output=False).fit_transform(AM_loaded_data['StimIDs'].reshape(-1, 1))\n",
    "elif region == 'ML':\n",
    "    X = ML_spike_mat\n",
    "    Z = OneHotEncoder(sparse_output=False).fit_transform(ML_loaded_data['StimIDs'].reshape(-1, 1))\n",
    "# Make targets one-hot since categorical\n",
    "\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits=n_splits, shuffle=False)\n",
    "decoder_params = tsao_df['decoder_args'][0]\n",
    "\n",
    "tsao_results = []\n",
    "\n",
    "# Average results across folds\n",
    "decoding_weights = []\n",
    "encoding_weights = []\n",
    "\n",
    "su_decoding_weights = []\n",
    "su_encoding_weights = []\n",
    "\n",
    "su_decoding_r2 = []\n",
    "su_encoding_r2 = []\n",
    "\n",
    "# Single unit statistics\n",
    "su_var = np.zeros((n_splits, X.shape[-1]))\n",
    "su_act = np.zeros((n_splits, X.shape[-1]))\n",
    "\n",
    "\n",
    "\n",
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        # Use time resolved responses to determine unit covariances, and use as much time as possible to estimate this.\n",
    "        if region == 'AM':\n",
    "            ccm_xtrain = list_of_arrays = [AM_sp_rates[i] for i in range(AM_sp_rates.shape[0])] # just pass this as list of trials\n",
    "            ccm = calc_cross_cov_mats_from_data(ccm_xtrain, T=AM_sp_rates.shape[1]-1)\n",
    "        elif region == 'ML':\n",
    "            ccm_xtrain = list_of_arrays = [ML_sp_rates[i] for i in range(ML_sp_rates.shape[0])] # just pass this as list of trials\n",
    "            ccm = calc_cross_cov_mats_from_data(ccm_xtrain, T=ML_sp_rates.shape[1]-1)\n",
    "        \n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, decodingregressor = logreg_decoder(xtest, xtrain, ztest, ztrain)\n",
    "        _, encodingregressor = categorical_reg(xtest, xtrain, ztest, ztrain) # Use categorical regression for the encoder model\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        \n",
    "        su_dw = []     # Single Unit Decoding Weights\n",
    "        su_ew = []     # Single Unit Encoding Weights\n",
    "        su_dec_r2 = [] # (McFadden's) R^2 value of predicting stimID from neural activity\n",
    "        su_enc_r2 = [] #              R^2 value of predicting neural activity from stimID\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_decoding, decReg = logreg_decoder(xtest, xtrain, ztest, ztrain)\n",
    "            su_dw.append(decReg.coef_)\n",
    "            su_dec_r2.append(r2_decoding)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding, encReg = categorical_reg(xtest, xtrain, ztest, ztrain)\n",
    "            su_ew.append(encReg.coef_)        \n",
    "            su_enc_r2.append(r2_encoding)\n",
    "\n",
    "\n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_decoding_r2.append(np.array(su_dec_r2))\n",
    "        su_encoding_r2.append(np.array(su_enc_r2))\n",
    "\n",
    "        \n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            \n",
    "            ccm_j = ccm[:, neu_idx, neu_idx].numpy()\n",
    "            ccm_j /= ccm_j[0]\n",
    "\n",
    "            thr = 1e-1\n",
    "            acov_crossing = np.where(ccm_j < thr)\n",
    "            if len(acov_crossing[0]) > 0:\n",
    "                su_act[fold_idx, neu_idx] = np.where(ccm_j < thr)[0][0]\n",
    "            else:\n",
    "                su_act[fold_idx, neu_idx] = len(ccm)\n",
    "\n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "        print(f\"Done with {fold_idx+1} fold\")\n",
    "\n",
    "\n",
    "\n",
    "# Average results across folds and save\n",
    "decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "\n",
    "su_decoding_r2 = np.mean(np.array(su_decoding_r2), axis=0)\n",
    "su_encoding_r2 = np.mean(np.array(su_encoding_r2), axis=0)\n",
    "\n",
    "su_var = np.mean(su_var, axis=0)\n",
    "su_act = np.mean(su_act, axis=0)\n",
    "\n",
    "result = {}\n",
    "for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_decoding_r2'\n",
    "                 , 'su_encoding_r2', 'su_var', 'su_act', 'decoder_params'):\n",
    "    result[variable] = eval(variable)\n",
    "\n",
    "tsao_results.append(result)\n",
    "\n",
    "with open(f'{savePath}/tsao_su_calcs_{region}.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(tsao_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
