{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for runing the FBCCA Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Get data into appropriate format\n",
    "    - For a new dataset, run preLoadTsao.py to split a given recording into each paradigm (stimulus set that was run), which yields a data structure that can be accessed by loaders.py\n",
    "        - Optionally, if this is a new paradigm, create a subfolder for all such data. E.g. create an 'FOB' folder for the Face-Object-Bodies stimuli by creating '/home/marcush/Data/TsaoLabData/split/FOB'. Move the split data into this subfolder\n",
    "    - In this subfolder create a new 'preloaded' folder that will save each of these split data as a preloaded struct (where each has its own load parameters, e.g. the bin size used, and area recorded from)\n",
    "1) Create submitfile for dimensionality reduction\n",
    "    - In VS Studio, go to Neural_Control/submit_files and find tsao_dimreduc_args.py\n",
    "    - Set the data path accordingly and set dimensionality reduction parameters (see other examples commented out)\n",
    "    - Run this script\n",
    "2) Submit this file to generate a folder for the outputs\n",
    "    - go to Neural_Control/runAnalysis.py\n",
    "    - change path to the suubmit_file you just edited, and give this run a name\n",
    "    - Run this script\n",
    "3) Perform the run\n",
    "    - Navigate to /home/marcush/Data/TsaoLabData/neural_control_output and find the run folder you've created (cd into that)\n",
    "    - activate the ncontrol conda env\n",
    "    - run: \"chmod 777 ./sbatch_resume.sh\"\n",
    "    - run: \"nohup ./sbatch_resume.sh &\"\n",
    "        - This runs the script in the background and creates a log file for the scripts output\n",
    "        - can run \"cat nohup.txt\" or \"nano nohup.txt\" to check the script progress\n",
    "\n",
    "\n",
    "\n",
    "4) Now perform decoding (optionally skip to step 7)\n",
    "    - In VS Studio, go to Neural_Control/submit_files and find tsao_decoding_args.py\n",
    "    - edit paths accordingly\n",
    "    - run script\n",
    "5) Submit this file to generate a folder for the outputs\n",
    "    - go to Neural_Control/runAnalysis.py\n",
    "    - change path to the suubmit_file you just edited, and give this run a name\n",
    "    - Run this script\n",
    "6) Perform the run\n",
    "    - Navigate to /home/marcush/Data/TsaoLabData/neural_control_output and find the run folder you've created (cd into that)\n",
    "    - activate the ncontrol conda env\n",
    "    - run: \"chmod 777 ./sbatch_resume.sh\"\n",
    "    - run: \"nohup ./sbatch_resume.sh &\"\n",
    "        - This runs the script in the background and creates a log file for the scripts output\n",
    "        - can run \"cat nohup.txt\" or \"nano nohup.txt\" to check the script progress\n",
    "\n",
    "\n",
    "\n",
    "7) Consolidate the output files\n",
    "    - go to Neural_Control/consolidateTsao.py\n",
    "    - give paths to the output dimreduc and decoding folders \n",
    "    - Run this script to get agglomerated files (used for actual analyses)\n",
    "\n",
    "8) Perform analyses!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
