{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import sys; sys.path.append(\"../..\")  # Allows access to all the scripts/modules in the larger directory\n",
    "from loaders import load_tsao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the dimreduc output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/marcush/Data/TsaoLabData/neural_control_output/degraded_small_batch/degraded_small_batch_0/dim_40_fold_0.dat'\n",
    "with open(path, 'rb') as f:\n",
    "    dat = pickle.load(f) \n",
    "\n",
    "df_dimreduc = pd.DataFrame(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find how many unique loader() calls there were in this batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashable(d):\n",
    "    \"\"\" Recursively convert a dictionary into a hashable type (tuples of tuples). \"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        return tuple((key, make_hashable(value)) for key, value in sorted(d.items()))\n",
    "    elif isinstance(d, list):\n",
    "        return tuple(make_hashable(value) for value in d)\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "# Assuming df_dimreduc['loader_args'] is your column with dictionaries\n",
    "unique_hashes = set(make_hashable(d) for d in df_dimreduc['loader_args'])\n",
    "\n",
    "# Convert each hashable entity back to a dictionary if necessary\n",
    "unique_dicts = [dict(t) for t in unique_hashes]  # This step might need adjustment based on your data structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data with these params, and save with the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = df_dimreduc['data_path'][0] + '/' + df_dimreduc['data_file'][0]\n",
    "output_dir = df_dimreduc['data_path'][0] + '/loader_data/'\n",
    "\n",
    "for d in unique_dicts:\n",
    "    bin_width = d['bin_width']\n",
    "    boxcox = d['boxcox']\n",
    "    filter_fn = d['filter_fn']\n",
    "    filter_kwargs = d['filter_kwargs']\n",
    "    region = d['region']\n",
    "\n",
    "    save_name = f\"{df_dimreduc['data_file'][0]}_{bin_width}_{region}.pickle\"\n",
    "    output_path = output_dir + save_name\n",
    "\n",
    "    dat = load_tsao(data_path, bin_width=bin_width, region=region, boxcox=boxcox, filter_fn=filter_fn, filter_kwargs=filter_kwargs)\n",
    "\n",
    "    with open(output_path, 'wb') as file:\n",
    "        pickle.dump(dat, file)\n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
