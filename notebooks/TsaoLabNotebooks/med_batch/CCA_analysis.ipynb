{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as glob\n",
    "import os\n",
    "import sys; sys.path.append(\"../../..\")  # Allows access to all the scripts/modules in the larger directory\n",
    "from utils import calc_loadings\n",
    "from collections import defaultdict\n",
    "from sklearn.cross_decomposition import CCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load consolidated dimreduc dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/marcush/Data/TsaoLabData/neural_control_output/degraded_decoding_med_batch/degraded_decoding_med_batch_glom.pickle'\n",
    "with open(path, 'rb') as f:\n",
    "    dat = pickle.load(f) \n",
    "df_dimreduc = pd.DataFrame(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dim', 'fold_idx', 'train_idxs', 'test_idxs', 'dimreduc_method',\n",
       "       'dimreduc_args', 'coef', 'score', 'predictions', 'loss',\n",
       "       'decoding_object', 'Xtrain', 'Xtest', 'Ytrain', 'Ytest', 'decoder',\n",
       "       'decoder_args', 'thresholds', 'data_file', 'loader', 'loader_args',\n",
       "       'task_args', 'data_path', 'results_file'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dimreduc.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the processed data from both regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashable(d):\n",
    "    return tuple(sorted((key, make_hashable(value)) if isinstance(value, dict) else (key, value)\n",
    "                        for key, value in d.items()))\n",
    "\n",
    "\n",
    "unique_hashes = set(make_hashable(d) for d in df_dimreduc['loader_args'])\n",
    "unique_dicts = [dict(u) for u in unique_hashes]\n",
    "\n",
    "for u in unique_dicts:\n",
    "    u['data_path'] = df_dimreduc['data_path'][0] + \"/\" + df_dimreduc['data_file'][0]\n",
    "    u['spike_threshold'] = None\n",
    "    u['trial_threshold'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_dict_path = df_dimreduc['data_path'][0] + \"/preloaded/preloadDict.pickle\"\n",
    "\n",
    "with open(preload_dict_path, 'rb') as file:\n",
    "    preloadDict = pickle.load(file)\n",
    "\n",
    "\n",
    "for arg_dict in unique_dicts:\n",
    "    arg_tuple = tuple(sorted(arg_dict.items()))\n",
    "\n",
    "\n",
    "    for args in preloadDict.keys():\n",
    "\n",
    "        if args == arg_tuple:\n",
    "\n",
    "            preloadID = preloadDict[arg_tuple]\n",
    "            loaded_data_path = os.path.dirname(preload_dict_path) + f\"/preloaded_data_{preloadID}.pickle\"\n",
    "            \n",
    "            if arg_dict['region'] == 'AM':\n",
    "                with open(loaded_data_path, 'rb') as file:\n",
    "                    AM_loaded_data = pickle.load(file)\n",
    "\n",
    "            elif arg_dict['region'] == 'ML':\n",
    "                with open(loaded_data_path, 'rb') as file:\n",
    "                    ML_loaded_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum spike rate per trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_spikes = np.sum(AM_loaded_data['spike_rates'], 1)\n",
    "ML_spikes = np.sum(ML_loaded_data['spike_rates'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcush/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/sklearn/cross_decomposition/_pls.py:113: ConvergenceWarning: Maximum number of iterations reached\n",
      "  warnings.warn(\"Maximum number of iterations reached\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "manual_CCA_dim = np.inf\n",
    "\n",
    "cca_dims = np.min((manual_CCA_dim, AM_spikes.shape[-1], ML_spikes.shape[-1])).astype(int)\n",
    "\n",
    "ccamodel = CCA(n_components=cca_dims)\n",
    "ccamodel.fit(ML_spikes, AM_spikes)\n",
    "\n",
    "cca_save_path = os.path.dirname(preload_dict_path) + f\"/CCA_{cca_dims}_dims.pickle\"\n",
    "\n",
    "with open(cca_save_path, 'wb') as file:\n",
    "    pickle.dump(ccamodel, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
