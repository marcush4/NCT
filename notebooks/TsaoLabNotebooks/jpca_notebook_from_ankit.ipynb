{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import time\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from mpl_toolkits.axisartist.axislines import AxesZero\n",
    "\n",
    "from dca.methods_comparison import JPCA #, symmJPCA\n",
    "#from pyuoi.linear_model.var  import VAR\n",
    "from neurosim.models.var import form_companion\n",
    "\n",
    "import sys; sys.path.append(\"../..\")  # Allows access to all the scripts/modules in the larger directory\n",
    "from utils import apply_df_filters, calc_loadings\n",
    "from loaders import load_sabes\n",
    "from segmentation import reach_segment_sabes, measure_straight_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSIDER REPLACING DEFAULT (STRANGE) SPIKE_RATES WITH THE ONES I COMPUTED MANUALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/marcush/Data/TsaoLabData/neural_control_output/degraded_dimreduc_param_search/degraded_dimreduc_param_search_glom.pickle'\n",
    "with open(path, 'rb') as f:\n",
    "    dat = pickle.load(f) \n",
    "\n",
    "df_dimreduc = pd.DataFrame(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashable(d):\n",
    "    return tuple(sorted((key, make_hashable(value)) if isinstance(value, dict) else (key, value)\n",
    "                        for key, value in d.items()))\n",
    "\n",
    "\n",
    "unique_hashes = set(make_hashable(d) for d in df_dimreduc['loader_args'])\n",
    "unique_dicts = [dict(u) for u in unique_hashes]\n",
    "\n",
    "for u in unique_dicts:\n",
    "    u['data_path'] = df_dimreduc['data_path'][0] + \"/\" + df_dimreduc['data_file'][0]\n",
    "    u['spike_threshold'] = None\n",
    "    u['trial_threshold'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_dict_path = df_dimreduc['data_path'][0] + \"/preloaded/preloadDict.pickle\"\n",
    "\n",
    "with open(preload_dict_path, 'rb') as file:\n",
    "    preloadDict = pickle.load(file)\n",
    "\n",
    "\n",
    "for arg_dict in unique_dicts:\n",
    "    arg_tuple = tuple(sorted(arg_dict.items()))\n",
    "\n",
    "\n",
    "    for args in preloadDict.keys():\n",
    "\n",
    "        if args == arg_tuple:\n",
    "\n",
    "            preloadID = preloadDict[arg_tuple]\n",
    "            loaded_data_path = os.path.dirname(preload_dict_path) + f\"/preloaded_data_{preloadID}.pickle\"\n",
    "            \n",
    "            if arg_dict['region'] == 'AM':\n",
    "                with open(loaded_data_path, 'rb') as file:\n",
    "                    AM_loaded_data = pickle.load(file)\n",
    "                    AM_data_file = loaded_data_path\n",
    "\n",
    "            elif arg_dict['region'] == 'ML':\n",
    "                with open(loaded_data_path, 'rb') as file:\n",
    "                    ML_loaded_data = pickle.load(file)\n",
    "                    ML_data_file = loaded_data_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcs = True\n",
    "rot_trajectories = True\n",
    "dyn_range = True\n",
    "boxplots = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [df_dimreduc['data_file'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_Region = 'AM'\n",
    "DIM = 21\n",
    "jDIM = DIM - 1 # must be even for jPCA \n",
    "LD_args = {'bin_width': 25, 'filter_fn': 'none', 'filter_kwargs': {}, 'boxcox': 0.5, 'region': curr_Region, 'degraded': True}\n",
    "\n",
    "calcs = False\n",
    "\n",
    "if calcs:\n",
    "\n",
    "    resultsd3 = []\n",
    "\n",
    "    data_file = AM_data_file\n",
    "    dat = AM_loaded_data\n",
    "\n",
    "\n",
    "    for i, data_file in tqdm(enumerate(data_files)):\n",
    "\n",
    "        y = np.squeeze(dat['spike_rates'])\n",
    "        for dimreduc_method in ['LQGCA', 'PCA']:\n",
    "            df_ = apply_df_filters(df_dimreduc, data_file=data_file, fold_idx=0, dim=DIM, dimreduc_method=dimreduc_method, loader_args=LD_args)\n",
    "            if dimreduc_method == 'LQGCA':\n",
    "                df_ = apply_df_filters(df_, dimreduc_args={})\n",
    "\n",
    "            assert(df_.shape[0] == 1)\n",
    "            V = df_.iloc[0]['coef']\n",
    "            if dimreduc_method == 'PCA':\n",
    "                V = V[:, 0:DIM]        \n",
    "\n",
    "            # Project data\n",
    "            yproj = y @ V\n",
    "            #yproj = np.array([yproj[t0:t0+20] for t0, t1 in dat['transition_times'] if t1 - t0 > 21])\n",
    "\n",
    "            result_ = {}\n",
    "            result_['data_file'] = data_file\n",
    "            result_['dimreduc_method'] = dimreduc_method\n",
    "\n",
    "\n",
    "            # 3 fits: Look at symmetric vs. asymmetric portions of regression onto differences\n",
    "            jpca = JPCA(n_components=jDIM, mean_subtract=False)\n",
    "            jpca.fit(yproj)\n",
    "            \n",
    "            result_['jeig'] = jpca.eigen_vals_\n",
    "            yprojcent = yproj\n",
    "\n",
    "            # For each time step, calculate the least squares projection of the state vector onto the next step\n",
    "            #a = np.eye(jDIM)\n",
    "            #for ii in range(yprojcent.shape[1] - 1):\n",
    "            #    lmodel = LinearRegression(fit_intercept=False)\n",
    "            #    lmodel.fit(yprojcent[:, ii, :], yprojcent[:, ii + 1, :])\n",
    "            #    a = a @ lmodel.coef_\n",
    "\n",
    "            print('%s\\n' % dimreduc_method)\n",
    "            #print(np.abs(np.linalg.eigvals(a)))\n",
    "\n",
    "            dyn_range = np.array([np.max(np.abs(y_)[:, j]) for y_ in yprojcent for j in range(jDIM)])\n",
    "            result_['dyn_range'] = np.mean(dyn_range)\n",
    "\n",
    "            resultsd3.append(result_)\n",
    "\n",
    "        with open(f'./jpca_{curr_Region}_projs.dat', 'wb') as f:\n",
    "            f.write(pickle.dumps(resultsd3))            \n",
    "else:\n",
    "    with open(f'./jpca_{curr_Region}_projs.dat', 'rb') as f:\n",
    "        resultsd3 = pickle.load(f)\n",
    "\n",
    "A_df = pd.DataFrame(resultsd3)\n",
    "\n",
    "d_U = np.zeros((len(data_files), 2, 3))\n",
    "maxim = np.zeros((len(data_files), 2, 3))\n",
    "\n",
    "\n",
    "with open(f'./jpca_{curr_Region}_randomcontrol.dat', 'rb') as f:\n",
    "    control_results = pickle.load(f)\n",
    "controldf = pd.DataFrame(control_results)\n",
    "\n",
    "control_reps = len(controldf['inner_rep'])\n",
    "\n",
    "maxim_control = np.zeros((len(data_files), control_reps, 3))\n",
    "\n",
    "for i in range(len(data_files)):\n",
    "    for j, dimreduc_method in enumerate(['LQGCA', 'PCA']):\n",
    "        df_ = apply_df_filters(A_df, data_file=data_files[i], dimreduc_method=dimreduc_method)\n",
    "        \n",
    "        eigs = df_.iloc[0]['jeig']\n",
    "\n",
    "        maxim[i, j, 0] = np.sum(np.abs(eigs))/2\n",
    "\n",
    "        maxim[i, j, 1] = np.sum(np.abs(eigs))/2\n",
    "        maxim[i, j, 2] = df_.iloc[0]['dyn_range']\n",
    "\n",
    "    for j in range(maxim_control.shape[1]):\n",
    "        df_ = apply_df_filters(controldf, data_file=data_files[i], inner_rep=j)\n",
    "        assert(df_.shape[0] == 1)\n",
    "\n",
    "        eigs = df_.iloc[0]['jeig']\n",
    "        maxim_control[i, j, 0] = np.sum(np.abs(eigs))/2\n",
    "        maxim_control[i, j, 1] = np.sum(np.abs(eigs))/2\n",
    "        eigs = df_.iloc[0]['dyn_range']\n",
    "        maxim_control[i, j, 2] = eigs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AM_loaded_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rot_trajectories:\n\u001b[1;32m     11\u001b[0m     data_file \u001b[38;5;241m=\u001b[39m data_files[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m     dat \u001b[38;5;241m=\u001b[39m \u001b[43mAM_loaded_data\u001b[49m\n\u001b[1;32m     14\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m apply_df_filters(df_dimreduc, data_file\u001b[38;5;241m=\u001b[39mdata_file, fold_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m=\u001b[39mDIM, dimreduc_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPCA\u001b[39m\u001b[38;5;124m'\u001b[39m, loader_args\u001b[38;5;241m=\u001b[39mLD_args)\n\u001b[1;32m     15\u001b[0m     df2 \u001b[38;5;241m=\u001b[39m apply_df_filters(df_dimreduc, data_file\u001b[38;5;241m=\u001b[39mdata_file, fold_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m=\u001b[39mDIM, dimreduc_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLQGCA\u001b[39m\u001b[38;5;124m'\u001b[39m, loader_args\u001b[38;5;241m=\u001b[39mLD_args)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AM_loaded_data' is not defined"
     ]
    }
   ],
   "source": [
    "curr_Region = 'AM'\n",
    "DIM = 21\n",
    "jDIM = DIM - 1 # must be even for jPCA \n",
    "LD_args = {'bin_width': 25, 'filter_fn': 'none', 'filter_kwargs': {}, 'boxcox': 0.5, 'region': curr_Region, 'degraded': True}\n",
    "figpath = \"./figs\"\n",
    "rot_trajectories = True\n",
    "\n",
    "\n",
    "if rot_trajectories:\n",
    "\n",
    "    data_file = data_files[0]\n",
    "    \n",
    "    dat = AM_loaded_data\n",
    "    df1 = apply_df_filters(df_dimreduc, data_file=data_file, fold_idx=0, dim=DIM, dimreduc_method='PCA', loader_args=LD_args)\n",
    "    df2 = apply_df_filters(df_dimreduc, data_file=data_file, fold_idx=0, dim=DIM, dimreduc_method='LQGCA', loader_args=LD_args)\n",
    "\n",
    "\n",
    "    x = dat['spike_rates']\n",
    "    xpca = x @ df1.iloc[0]['coef'][:, 0:jDIM]\n",
    "    xdca = x @ df2.iloc[0]['coef']\n",
    "\n",
    "    jpca1 = JPCA(n_components=jDIM, mean_subtract=False)\n",
    "    jpca1.fit(xpca)\n",
    "\n",
    "    jpca2 = JPCA(n_components=jDIM, mean_subtract=False)\n",
    "    jpca2.fit(xdca)\n",
    "\n",
    "    xpca_j = jpca1.transform(xpca)\n",
    "    xdca_j = jpca2.transform(xdca)\n",
    "\n",
    "\n",
    "    xpca_j_avg = np.squeeze(np.mean(xpca_j, 0))\n",
    "    xdca_j_avg = np.squeeze(np.mean(xdca_j, 0))\n",
    "\n",
    "\n",
    "    ############## Trajectory Plots #################\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1991, 39, 20)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(xpca_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77649, 371)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x.reshape(-1, x.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AM_loaded_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rot_trajectories:\n\u001b[1;32m      9\u001b[0m     data_file \u001b[38;5;241m=\u001b[39m data_files[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m     dat \u001b[38;5;241m=\u001b[39m \u001b[43mAM_loaded_data\u001b[49m\n\u001b[1;32m     12\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m apply_df_filters(df_dimreduc, data_file\u001b[38;5;241m=\u001b[39mdata_file, fold_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m=\u001b[39mDIM, dimreduc_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPCA\u001b[39m\u001b[38;5;124m'\u001b[39m, loader_args\u001b[38;5;241m=\u001b[39mLD_args)\n\u001b[1;32m     13\u001b[0m     df2 \u001b[38;5;241m=\u001b[39m apply_df_filters(df_dimreduc, data_file\u001b[38;5;241m=\u001b[39mdata_file, fold_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m=\u001b[39mDIM, dimreduc_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLQGCA\u001b[39m\u001b[38;5;124m'\u001b[39m, loader_args\u001b[38;5;241m=\u001b[39mLD_args)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AM_loaded_data' is not defined"
     ]
    }
   ],
   "source": [
    "curr_Region = 'AM'\n",
    "DIM = 21\n",
    "jDIM = DIM - 1 # must be even for jPCA \n",
    "LD_args = {'bin_width': 25, 'filter_fn': 'none', 'filter_kwargs': {}, 'boxcox': 0.5, 'region': curr_Region, 'degraded': True}\n",
    "figpath = \"./figs\"\n",
    "\n",
    "if rot_trajectories:\n",
    "\n",
    "    data_file = data_files[0]\n",
    "    \n",
    "    dat = AM_loaded_data\n",
    "    df1 = apply_df_filters(df_dimreduc, data_file=data_file, fold_idx=0, dim=DIM, dimreduc_method='PCA', loader_args=LD_args)\n",
    "    df2 = apply_df_filters(df_dimreduc, data_file=data_file, fold_idx=0, dim=DIM, dimreduc_method='LQGCA', loader_args=LD_args)\n",
    "\n",
    "\n",
    "    x = dat['spike_rates']\n",
    "    #x = x.reshape(-1, x.shape[2])\n",
    "\n",
    "    xpca = x @ df1.iloc[0]['coef'][:, 0:jDIM]\n",
    "    xdca = x @ df2.iloc[0]['coef']\n",
    "\n",
    "    jpca1 = JPCA(n_components=jDIM-2, mean_subtract=False)\n",
    "    jpca1.fit(xpca)\n",
    "\n",
    "    jpca2 = JPCA(n_components=jDIM, mean_subtract=False)\n",
    "    jpca2.fit(xdca)\n",
    "\n",
    "    xpca_j = jpca1.transform(xpca)\n",
    "    xdca_j = jpca2.transform(xdca)\n",
    "\n",
    "    print(xpca_j.shape)\n",
    "\n",
    "    #xpca_j = xpca_j.reshape(-1, xpca_j.shape[2])\n",
    "    #xdca_j = xdca_j.reshape(-1, xdca_j.shape[2])\n",
    "\n",
    "    ## Measure the straight_dev of the projected neural data\n",
    "    #pca_straightdev = np.zeros(len(dat['target_pairs']))\n",
    "    #dca_straightdev = np.zeros(len(dat['target_pairs']))\n",
    "    #transition_times = dat['transition_times']\n",
    "\n",
    "    pca_straightdev = np.zeros(x.shape[0])\n",
    "    dca_straightdev = np.zeros(x.shape[0])\n",
    "    transition_times = []\n",
    "    trial_dur = dat['spike_rates'].shape[1]\n",
    "    for trial in np.arange(dat['spike_rates'].shape[0]):\n",
    "        transition_times.append([trial*trial_dur, trial*trial_dur + trial_dur])\n",
    "\n",
    "    #for i in range(len(dat['target_pairs'])):\n",
    "    for i in range(x.shape[0]):\n",
    "        \n",
    "        #trajectory = gaussian_filter1d(xpca_j[0, transition_times[i][0]:transition_times[i][1]],  sigma=5, axis=0)\n",
    "        trajectory = gaussian_filter1d(xpca_j[0, transition_times[i][0]:transition_times[i][1]],  sigma=5, axis=0)\n",
    "\n",
    "        trajectory -= trajectory[0]\n",
    "        \n",
    "        start = trajectory[0, :]\n",
    "        end = trajectory[-1, :]\n",
    "        \n",
    "        pca_straightdev[i] = measure_straight_dev(trajectory, start, end)\n",
    "\n",
    "        trajectory = gaussian_filter1d(xdca_j[0, transition_times[i][0]:transition_times[i][1]],  sigma=5, axis=0)\n",
    "        trajectory -= trajectory[0]\n",
    "        \n",
    "        start = trajectory[0, :]\n",
    "        end = trajectory[-1, :]\n",
    "        dca_straightdev[i] = measure_straight_dev(trajectory, start, end)\n",
    "        \n",
    " \n",
    "    pca_devorder = np.argsort(pca_straightdev)[::-1]\n",
    "    dca_devorder = np.argsort(dca_straightdev)[::-1]\n",
    "\n",
    "    ############## Trajectory Plots #################\n",
    "\n",
    "\n",
    "    # Save as two separate figures\n",
    "    fig1, ax1 = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    fig2, ax2 = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    ax = [ax1, ax2]\n",
    "\n",
    "    for i in range(0, 25):\n",
    "        \n",
    "        idx = dca_devorder[i]\n",
    "\n",
    "        # Plot only 20 timesteps\n",
    "        t0 = transition_times[idx][0]\n",
    "        t1 = min(transition_times[idx][0] + 40, transition_times[idx][1])\n",
    "\n",
    "        trajectory = gaussian_filter1d(xpca_j[0, t0:t1], sigma=5, axis=0)[:-3]\n",
    "\n",
    "        # Center and normalize trajectories\n",
    "        trajectory -= trajectory[0]\n",
    "        #trajectory /= np.linalg.norm(trajectory)\n",
    "\n",
    "        # Rotate trajectory so that the first 5 timesteps all go off at the same angle\n",
    "        theta0 = np.arctan2(trajectory[15, 1], trajectory[15, 0])\n",
    "\n",
    "        # Rotate *clockwise* by theta\n",
    "        R = lambda theta: np.array([[np.cos(-1*theta), -np.sin(-theta)], \\\n",
    "                                    [np.sin(-theta), np.cos(theta)]])        \n",
    "        trajectory = np.array([R(theta0 - np.pi/4) @ t[0:2] for t in trajectory])\n",
    "\n",
    "        ax[1].plot(trajectory[:, 0], trajectory[:, 1], 'k', alpha=0.5)\n",
    "        ax[1].arrow(trajectory[-1, 0], trajectory[-1, 1], \n",
    "                    trajectory[-1, 0] - trajectory[-2, 0], trajectory[-1, 1] - trajectory[-2, 1], \n",
    "                    head_width=0.08, color=\"k\", alpha=0.5)\n",
    "        \n",
    "        \n",
    "        idx = dca_devorder[i]\n",
    "        t0 = transition_times[idx][0]\n",
    "        t1 = min(transition_times[idx][0] + 40, transition_times[idx][1])\n",
    "        trajectory = gaussian_filter1d(xdca_j[0, t0:t1], sigma=5, axis=0)[:-3]\n",
    "\n",
    "        # Center trajectories\n",
    "        trajectory -= trajectory[0]\n",
    "        #trajectory /= np.linalg.norm(trajectory)\n",
    "\n",
    "        # Rotate trajectory so that the first 5 timesteps all go off at the same angle\n",
    "        theta0 = np.arctan2(trajectory[15, 1], trajectory[15, 0])\n",
    "\n",
    "        trajectory = np.array([R(theta0 - np.pi/4) @ t[0:2] for t in trajectory])\n",
    "\n",
    "        ax[0].plot(trajectory[:, 0], trajectory[:, 1], '#c73d34', alpha=0.5)\n",
    "        ax[0].arrow(trajectory[-1, 0], trajectory[-1, 1], \n",
    "                    trajectory[-1, 0] - trajectory[-2, 0], trajectory[-1, 1] - trajectory[-2, 1], \n",
    "                    head_width=0.05, color=\"#c73d34\", alpha=0.5)\n",
    "\n",
    "    _, p = scipy.stats.wilcoxon(maxim[:, 0, 2], maxim[:, 1, 2], alternative='less')\n",
    "    print('Re p:%f' % p)\n",
    "\n",
    "\n",
    "    # ax[0].set_xticklabels([])\n",
    "    # ax[0].set_yticklabels([])\n",
    "    \n",
    "    # ax[1].set_xticklabels([])\n",
    "    # ax[1].set_yticklabels([])\n",
    "\n",
    "    ax[0].set_aspect('equal')   \n",
    "    ax[1].set_aspect('equal')\n",
    "    ax[1].set_xlim([-2.2, 3.5])\n",
    "    ax[1].set_ylim([-2.2, 3.5])\n",
    "\n",
    "    ax[0].set_xlim([-2.2, 3.5])\n",
    "    ax[0].set_ylim([-2.2, 3.5])\n",
    "\n",
    "    # ax[1].set_title('jPCA on PCA', fontsize=14)\n",
    "    # ax[1].set_ylabel('jPC2', fontsize=14)\n",
    "    # ax[1].set_xlabel('jPC1', fontsize=14)\n",
    "\n",
    "    # ax[0].set_title('jPCA on FCCA', fontsize=14)\n",
    "    # ax[0].set_ylabel('jPC2', fontsize=14)\n",
    "    # ax[0].set_xlabel('jPC1', fontsize=14)\n",
    "\n",
    "    ax[0].spines['right'].set_color('none')\n",
    "    ax[0].spines['top'].set_color('none')\n",
    "    ax[0].spines['left'].set_position('zero')\n",
    "    ax[0].spines['bottom'].set_position('zero')\n",
    "    ax[0].plot(2, 0, \">k\", clip_on=False)\n",
    "    ax[0].plot(0, 2, \"^k\", clip_on=False)\n",
    "    ax[0].spines['left'].set_bounds(0, 2)\n",
    "    ax[0].spines['bottom'].set_bounds(0, 2)\n",
    "    # ax[0].spines['left'].set_color('none')\n",
    "    # ax[0].spines['bottom'].set_color('none')\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "\n",
    "    ax[1].spines['right'].set_color('none')\n",
    "    ax[1].spines['top'].set_color('none')\n",
    "    ax[1].spines['left'].set_position('zero')\n",
    "    ax[1].spines['bottom'].set_position('zero')\n",
    "    ax[1].spines['left'].set_bounds(0, 2)\n",
    "    ax[1].spines['bottom'].set_bounds(0, 2)\n",
    "    ax[1].plot(2, 0, \">k\", clip_on=False)\n",
    "    ax[1].plot(0, 2, \"^k\", clip_on=False)\n",
    "\n",
    "    # ax[1].spines['left'].set_color('none')\n",
    "    # ax[1].spines['bottom'].set_color('none')\n",
    "    # ax[1].set_xticks([])\n",
    "    # ax[1].set_yticks([])\n",
    "    fig1.tight_layout()\n",
    "    fig1.savefig('%s/trajectories_a.pdf' % figpath, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    fig2.tight_layout()\n",
    "    fig2.savefig('%s/trajectories_b.pdf' % figpath, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1991, 39, 18)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xpca_j.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEGIN ORIGINAL CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = {'indy_20160426_01': 0,\n",
    "               'indy_20160622_01':1700,\n",
    "               'indy_20160624_03': 500,\n",
    "               'indy_20160627_01': 0,\n",
    "               'indy_20160630_01': 0,\n",
    "               'indy_20160915_01': 0,\n",
    "               'indy_20160921_01': 0,\n",
    "               'indy_20160930_02': 0,\n",
    "               'indy_20160930_05': 300,\n",
    "               'indy_20161005_06': 0,\n",
    "               'indy_20161006_02': 350,\n",
    "               'indy_20161007_02': 950,\n",
    "               'indy_20161011_03': 0,\n",
    "               'indy_20161013_03': 0,\n",
    "               'indy_20161014_04': 0,\n",
    "               'indy_20161017_02': 0,\n",
    "               'indy_20161024_03': 0,\n",
    "               'indy_20161025_04': 0,\n",
    "               'indy_20161026_03': 0,\n",
    "               'indy_20161027_03': 500,\n",
    "               'indy_20161206_02': 5500,\n",
    "               'indy_20161207_02': 0,\n",
    "               'indy_20161212_02': 0,\n",
    "               'indy_20161220_02': 0,\n",
    "               'indy_20170123_02': 0,\n",
    "               'indy_20170124_01': 0,\n",
    "               'indy_20170127_03': 0,\n",
    "               'indy_20170131_02': 0,\n",
    "               }\n",
    "\n",
    "\n",
    "calcs = True\n",
    "rot_trajectories = True\n",
    "dyn_range = True\n",
    "boxplots = True\n",
    "\n",
    "# Where to save?\n",
    "if len(sys.argv) > 1:\n",
    "    figpath = sys.argv[1]\n",
    "else:\n",
    "    #figpath = '/home/akumar/nse/neural_control/figs/loco_indy_merge'\n",
    "    figpath = './figs'\n",
    "\n",
    "\n",
    "with open('/mnt/Secondary/data/postprocessed/indy_decoding_df2.dat', 'rb') as f:\n",
    "    indy_df = pickle.load(f)\n",
    "indy_df = pd.DataFrame(indy_df)\n",
    "\n",
    "with open('/mnt/Secondary/data/postprocessed/loco_decoding_df.dat', 'rb') as f:\n",
    "    loco_df = pickle.load(f)\n",
    "loco_df = pd.DataFrame(loco_df)\n",
    "loco_df = apply_df_filters(loco_df,\n",
    "                        loader_args={'bin_width': 50, 'filter_fn': 'none', 'filter_kwargs': {}, 'boxcox': 0.5, 'spike_threshold': 100, 'region': 'M1'},\n",
    "                        decoder_args={'trainlag': 4, 'testlag': 4, 'decoding_window': 5})\n",
    "\n",
    "good_loco_files = ['loco_20170210_03.mat',\n",
    "'loco_20170213_02.mat',\n",
    "'loco_20170215_02.mat',\n",
    "'loco_20170227_04.mat',\n",
    "'loco_20170228_02.mat',\n",
    "'loco_20170301_05.mat',\n",
    "'loco_20170302_02.mat']\n",
    "\n",
    "loco_df = apply_df_filters(loco_df, data_file=good_loco_files)        \n",
    "\n",
    "sabes_df = pd.concat([loco_df, indy_df])\n",
    "\n",
    "data_files = np.unique(sabes_df['data_file'].values)\n",
    "dpath = '/mnt/Secondary/data/sabes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dim', 'fold_idx', 'train_idxs', 'test_idxs', 'dimreduc_method',\n",
       "       'dimreduc_args', 'coef', 'score', 'data_file', 'loader', 'loader_args',\n",
       "       'task_args', 'data_path', 'results_file'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dimreduc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 21\n",
    "if calcs:\n",
    "    # Now do subspace identification/VAR inference within these \n",
    "    # results = []\n",
    "    resultsd3 = []\n",
    "    for i, data_file in tqdm(enumerate(data_files)):\n",
    "        dpath = '/mnt/Secondary/data/sabes'\n",
    "        dat = load_sabes('%s/%s' % (dpath, data_file))\n",
    "        dat = reach_segment_sabes(dat, data_file=data_file.split('.mat')[0])\n",
    "        transition_times = dat['transition_times']\n",
    "        y = np.squeeze(dat['spike_rates'])\n",
    "        for dimreduc_method in ['LQGCA', 'PCA']:\n",
    "            df_ = apply_df_filters(sabes_df, data_file=data_file, fold_idx=0, dim=DIM, dimreduc_method=dimreduc_method)\n",
    "            if dimreduc_method == 'LQGCA':\n",
    "                df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 10})\n",
    "\n",
    "            assert(df_.shape[0] == 1)\n",
    "            V = df_.iloc[0]['coef']\n",
    "            if dimreduc_method == 'PCA':\n",
    "                V = V[:, 0:DIM]        \n",
    "\n",
    "            # Project data\n",
    "            yproj = y @ V\n",
    "            yproj = np.array([yproj[t0:t0+20] for t0, t1 in dat['transition_times'] if t1 - t0 > 21])\n",
    "\n",
    "            result_ = {}\n",
    "            result_['data_file'] = data_file\n",
    "            result_['dimreduc_method'] = dimreduc_method\n",
    "\n",
    "\n",
    "            # 3 fits: Look at symmetric vs. asymmetric portions of regression onto differences\n",
    "            jpca = JPCA(n_components=DIM, mean_subtract=False)\n",
    "            jpca.fit(yproj)\n",
    "            \n",
    "            result_['jeig'] = jpca.eigen_vals_\n",
    "            yprojcent = yproj\n",
    "\n",
    "            # For each time step, calculate the least squares projection of the state vector onto the next step\n",
    "            a = np.eye(DIM)\n",
    "            for ii in range(yprojcent.shape[1] - 1):\n",
    "                lmodel = LinearRegression(fit_intercept=False)\n",
    "                lmodel.fit(yprojcent[:, ii, :], yprojcent[:, ii + 1, :])\n",
    "                a = a @ lmodel.coef_\n",
    "\n",
    "            print('%s\\n' % dimreduc_method)\n",
    "            print(np.abs(np.linalg.eigvals(a)))\n",
    "\n",
    "            dyn_range = np.array([np.max(np.abs(y_)[:, j]) for y_ in yprojcent for j in range(DIM)])\n",
    "            result_['dyn_range'] = np.mean(dyn_range)\n",
    "\n",
    "            resultsd3.append(result_)\n",
    "\n",
    "    # with open('jpcaAtmp_il2.dat', 'wb') as f:\n",
    "    #     f.write(pickle.dumps(resultsd3))            \n",
    "else:\n",
    "    with open('jpcaAtmp_il2.dat', 'rb') as f:\n",
    "        resultsd3 = pickle.load(f)\n",
    "\n",
    "A_df = pd.DataFrame(resultsd3)\n",
    "\n",
    "d_U = np.zeros((len(data_files), 2, 3))\n",
    "maxim = np.zeros((len(data_files), 2, 3))\n",
    "\n",
    "d1 = []\n",
    "d2 = []\n",
    "\n",
    "with open('jpcaAtmp_randomcontrol2.dat', 'rb') as f:\n",
    "    control_results = pickle.load(f)\n",
    "controldf = pd.DataFrame(control_results)\n",
    "\n",
    "maxim_control = np.zeros((len(data_files), 1000, 3))\n",
    "\n",
    "for i in range(len(data_files)):\n",
    "    for j, dimreduc_method in enumerate(['LQGCA', 'PCA']):\n",
    "        df_ = apply_df_filters(A_df, data_file=data_files[i], dimreduc_method=dimreduc_method)\n",
    "        eigs = df_.iloc[0]['jeig']\n",
    "\n",
    "        maxim[i, j, 0] = np.sum(np.abs(eigs))/2\n",
    "\n",
    "        maxim[i, j, 1] = np.sum(np.abs(eigs))/2\n",
    "        maxim[i, j, 2] = df_.iloc[0]['dyn_range']\n",
    "\n",
    "    for j in range(maxim_control.shape[1]):\n",
    "        df_ = apply_df_filters(controldf, data_file=data_files[i], inner_rep=j)\n",
    "        assert(df_.shape[0] == 1)\n",
    "\n",
    "        eigs = df_.iloc[0]['jeig']\n",
    "        maxim_control[i, j, 0] = np.sum(np.abs(eigs))/2\n",
    "        maxim_control[i, j, 1] = np.sum(np.abs(eigs))/2\n",
    "        eigs = df_.iloc[0]['dyn_range']\n",
    "        maxim_control[i, j, 2] = eigs\n",
    "\n",
    "print(d1)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next up:\n",
    "# Rotational trajectories.\n",
    "if rot_trajectories:\n",
    "    # (5., 7., 10., 12.. 16. 17. 19. 20. 25..)\n",
    "    data_file = data_files[25]\n",
    "\n",
    "    df1 = apply_df_filters(sabes_df, data_file=data_file, fold_idx=0, dim=6, dimreduc_method='PCA')\n",
    "    df2 = apply_df_filters(sabes_df, data_file=data_file, fold_idx=0, dim=6, dimreduc_method='LQGCA', \n",
    "                        dimreduc_args={'T':3, 'loss_type':'trace', 'n_init':10})\n",
    "\n",
    "\n",
    "    datpath = '/mnt/Secondary/data/sabes'\n",
    "    dat = load_sabes('%s/%s' % (datpath, data_file))\n",
    "    dat = reach_segment_sabes(dat, data_file=data_file.split('.mat')[0])\n",
    "\n",
    "    # x = np.array([StandardScaler().fit_transform(dat['spike_rates'][j, ...]) \n",
    "    #             for j in range(dat['spike_rates'].shape[0])])\n",
    "    x = dat['spike_rates']\n",
    "    xpca = x @ df1.iloc[0]['coef'][:, 0:6]\n",
    "    xdca = x @ df2.iloc[0]['coef']\n",
    "\n",
    "    jpca1 = JPCA(n_components=6, mean_subtract=False)\n",
    "    jpca1.fit(xpca)\n",
    "\n",
    "    jpca2 = JPCA(n_components=6, mean_subtract=False)\n",
    "    jpca2.fit(xdca)\n",
    "\n",
    "    xpca_j = jpca1.transform(xpca)\n",
    "    xdca_j = jpca2.transform(xdca)\n",
    "\n",
    "\n",
    "    # Measure the straight_dev of the projected neural data\n",
    "    pca_straightdev = np.zeros(len(dat['target_pairs']))\n",
    "    dca_straightdev = np.zeros(len(dat['target_pairs']))\n",
    "    transition_times = dat['transition_times']\n",
    "    for i in range(len(dat['target_pairs'])):\n",
    "        \n",
    "        trajectory = gaussian_filter1d(xpca_j[0, transition_times[i][0]:transition_times[i][1]], \n",
    "                                    sigma=5, axis=0)\n",
    "        trajectory -= trajectory[0]\n",
    "        \n",
    "        start = trajectory[0, :]\n",
    "        end = trajectory[-1, :]\n",
    "        \n",
    "        pca_straightdev[i] = measure_straight_dev(trajectory, start, end)\n",
    "\n",
    "        trajectory = gaussian_filter1d(xdca_j[0, transition_times[i][0]:transition_times[i][1]], \n",
    "                                    sigma=5, axis=0)\n",
    "        trajectory -= trajectory[0]\n",
    "        \n",
    "        start = trajectory[0, :]\n",
    "        end = trajectory[-1, :]\n",
    "        dca_straightdev[i] = measure_straight_dev(trajectory, start, end)\n",
    "        \n",
    "\n",
    "    #pca_devorder = np.arange(pca_straightdev.size)\n",
    "    #dca_devorder = np.arange(dca_straightdev.size)    \n",
    "    pca_devorder = np.argsort(pca_straightdev)[::-1]\n",
    "    dca_devorder = np.argsort(dca_straightdev)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datpath = '/mnt/Secondary/data/sabes'\n",
    "dat = load_sabes('%s/%s' % (datpath, data_file))\n",
    "dat = reach_segment_sabes(dat, data_file=data_file.split('.mat')[0])\n",
    "\n",
    "\n",
    "......\n",
    "\n",
    "pca_straightdev = np.zeros(len(dat['target_pairs']))\n",
    "dca_straightdev = np.zeros(len(dat['target_pairs']))\n",
    "transition_times = dat['transition_times']\n",
    "for i in range(len(dat['target_pairs'])):\n",
    "    \n",
    "    trajectory = gaussian_filter1d(xpca_j[0, transition_times[i][0]:transition_times[i][1]], \n",
    "                                sigma=5, axis=0)\n",
    "    trajectory -= trajectory[0]\n",
    "    \n",
    "    start = trajectory[0, :]\n",
    "    end = trajectory[-1, :]\n",
    "    \n",
    "    pca_straightdev[i] = measure_straight_dev(trajectory, start, end)\n",
    "\n",
    "    trajectory = gaussian_filter1d(xdca_j[0, transition_times[i][0]:transition_times[i][1]], \n",
    "                                sigma=5, axis=0)\n",
    "    trajectory -= trajectory[0]\n",
    "    \n",
    "    start = trajectory[0, :]\n",
    "    end = trajectory[-1, :]\n",
    "    dca_straightdev[i] = measure_straight_dev(trajectory, start, end)\n",
    "\n",
    "pca_devorder = np.argsort(pca_straightdev)[::-1]\n",
    "dca_devorder = np.argsort(dca_straightdev)[::-1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Trajectory Amplification #################\n",
    "if dyn_range:\n",
    "    for didx, data_file in enumerate(data_files):\n",
    "        datpath = '/mnt/Secondary/data/sabes'\n",
    "        dat = load_sabes('%s/%s' % (datpath, data_file))\n",
    "        dat = reach_segment_sabes(dat, data_file=data_file.split('.mat')[0])\n",
    "\n",
    "        y = np.squeeze(dat['spike_rates'])\n",
    "\n",
    "        colors = ['k', 'r']\n",
    "        for j, dimreduc_method in enumerate(['PCA', 'LQGCA']):\n",
    "            df_ = apply_df_filters(sabes_df, data_file=data_file, fold_idx=0, dim=DIM, dimreduc_method=dimreduc_method)\n",
    "            if dimreduc_method == 'LQGCA':\n",
    "                df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 10})\n",
    "\n",
    "            assert(df_.shape[0] == 1)\n",
    "            V = df_.iloc[0]['coef']\n",
    "            if dimreduc_method == 'PCA':\n",
    "                V = V[:, 0:DIM]        \n",
    "\n",
    "            # Project data\n",
    "            yproj = y @ V\n",
    "            #yproj = np.array([yproj[t0:t0+40] for t0, t1 in dat['transition_times'] if t1 - t0 > 40])\n",
    "            yproj = np.array([y_ - y_[0] for y_ in yproj])\n",
    "            dY = np.concatenate(np.diff(yproj, axis=1), axis=0)\n",
    "            Y_prestate = np.concatenate(yproj[:, :-1], axis=0)\n",
    "\n",
    "            # Least squares\n",
    "            A, _, _, _ = np.linalg.lstsq(Y_prestate, dY, rcond=None)\n",
    "            _, s, _ = np.linalg.svd(A)\n",
    "            print('%s' % dimreduc_method + s)\n",
    "\n",
    "            # Identify the directions in which there is the most amplification over multiple timesteps\n",
    "            # Project the data along those directions and also record the amplification implied by the model\n",
    "        \n",
    "            # Iterate the lyapunov equation for 10 timesteps\n",
    "            P = np.zeros((DIM, DIM))\n",
    "            for _ in range(10):\n",
    "                dP = A @ P + P @ A.T + np.eye(DIM)\n",
    "                P += dP\n",
    "\n",
    "            eig, U = np.linalg.eig(P)\n",
    "            # eig, U = np.linalg.eig(scipy.linalg.expm(A.T) @ scipy.linalg.expm(A))\n",
    "            eig = np.sort(eig)[::-1]\n",
    "            U = U[:, np.argsort(eig)[::-1]]\n",
    "            U = U[:, 0:2]\n",
    "            # Plot smoothed, centered trajectories for all reaches in top 2 dimensions\n",
    "\n",
    "            # Argsort by the maximum amplitude in the top 2 dimensions\n",
    "            trajectory = gaussian_filter1d(yproj, sigma=5, axis=1)\n",
    "            trajectory -= trajectory[:, 0:1, :]\n",
    "            trajectory = trajectory @ U\n",
    "            dyn_range = np.max(np.abs(trajectory), axis=1)\n",
    "            ordering = np.argsort(dyn_range, axis=0)[::-1]\n",
    "\n",
    "            t0 = trajectory[ordering[:, 0], :, 0]\n",
    "            t1 = trajectory[ordering[:, 1], :, 1]\n",
    "\n",
    "            f1, a1 = plt.subplots(1, 1, figsize=(4.2, 4))\n",
    "            f2, a2 = plt.subplots(1, 1, figsize=(4.2, 4))\n",
    "            ax = [a1, a2]\n",
    "\n",
    "            for i in range(min(50, t0.shape[0])):\n",
    "                ax[0].plot(50 * np.arange(40), t0[i], color=colors[j], alpha=0.5, linewidth=1.5)\n",
    "                ax[1].plot(50 * np.arange(40), t1[i], color=colors[j], alpha=0.5, linewidth=1.5)\n",
    "                #ax[2*j].set_title(np.sum(eig))\n",
    "                \n",
    "            for a in ax:\n",
    "                a.spines['bottom'].set_position('center')\n",
    "                # Eliminate upper and right axes\n",
    "                a.spines['right'].set_color('none')\n",
    "                a.spines['top'].set_color('none')\n",
    "\n",
    "                # Show ticks in the left and lower axes only\n",
    "                a.xaxis.set_ticks_position('bottom')\n",
    "                a.yaxis.set_ticks_position('left')\n",
    "\n",
    "                a.set_xticks([0, 2])\n",
    "                a.set_xticklabels([])\n",
    "                a.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "                a.set_xlabel('Time (s)', fontsize=12)\n",
    "                a.xaxis.set_label_coords(1.1, 0.56)\n",
    "                \n",
    "            # Set y scale according to the current yscale on PCA 0\n",
    "            if j == 0:\n",
    "                ylim_max = np.max(np.abs(t0[0])) + 0.25\n",
    "                ylim = [-ylim_max, ylim_max]\n",
    "\n",
    "            for a in ax:\n",
    "                a.set_ylim(ylim)\n",
    "                a.set_yticks([-int(ylim_max), 0, int(ylim_max)])\n",
    "                a.set_ylabel('Amplitude (a.u.)', fontsize=12)\n",
    "\n",
    "            if j == 0:\n",
    "                ax[0].set_title('FFC Component 1', fontsize=12)\n",
    "                ax[1].set_title('FFC Component 2', fontsize=12)\n",
    "            else:\n",
    "                ax[0].set_title('FBC Component 1', fontsize=12)\n",
    "                ax[1].set_title('FBC Component 2', fontsize=12)\n",
    "\n",
    "            #f1.tight_layout()\n",
    "            #f2.tight_layout()\n",
    "            f1.savefig('/home/akumar/nse/neural_control/figs/amplification/%d_e_%s1.pdf' % (didx, dimreduc_method), bbox_inches='tight', pad_inches=0)\n",
    "            f2.savefig('/home/akumar/nse/neural_control/figs/amplification/%d_e_%s2.pdf' % (didx, dimreduc_method), bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if boxplots:\n",
    "    # Boxplots\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(6, 3))\n",
    "    medianprops = dict(linewidth=1, color='b')\n",
    "    whiskerprops=dict(linewidth=0)\n",
    "    #bplot = ax.boxplot([d_U[:, 2, 1], d_U[:, 3, 1]], patch_artist=True, medianprops=medianprops, notch=True, vert=False, showfliers=False)\n",
    "    # Plot relative to the control...test for difference from zero\n",
    "\n",
    "    # Center relative to random - per recording session\n",
    "    mu = np.mean(maxim_control[..., 1], axis=1)\n",
    "    sigma = np.std(maxim_control[..., 1], axis=1)\n",
    "    r1 = maxim[:, 0, 1] - mu\n",
    "    r2 = maxim[:, 1, 1] - mu\n",
    "\n",
    "    bplot = ax[0].boxplot([r1, r2], patch_artist=True, \n",
    "                    medianprops=medianprops, notch=False, vert=False, showfliers=False, widths=[0.3, 0.3],\n",
    "                    whiskerprops=whiskerprops, showcaps=False)\n",
    "\n",
    "    # _, p = scipy.stats.wilcoxon(d_U[:, 2, 1], d_U[:, 3, 1])\n",
    "    _, p = scipy.stats.wilcoxon(maxim[:, 0, 1], maxim[:, 1, 1], alternative='greater')\n",
    "    print('Im p:%f' % p)\n",
    "\n",
    "    # test that each is stochastically greater than the median random\n",
    "    x1 = maxim[:, 0, 1] - np.median(maxim_control[..., 1].ravel())\n",
    "    x2 = maxim[:, 1, 1] - np.median(maxim_control[..., 1].ravel())\n",
    "\n",
    "    _, p1 = scipy.stats.wilcoxon(x1, alternative='greater')\n",
    "    _, p2 = scipy.stats.wilcoxon(x2, alternative='greater')\n",
    "\n",
    "    # _, p = scipy.stats.wilcoxon(maxim[:, 0, 2], maxim[:, 1, 2], alternative='less')\n",
    "    # print('Re p:%f' % p)\n",
    "\n",
    "    # Mutliple comparison adjusted test of maxim control against PCA and FCCA\n",
    "    # _, p1 = scipy.stats.mannwhitneyu(maxim[:, 0, 1], maxim_control[..., 1].ravel(), alternative='greater')\n",
    "    # _, p2 = scipy.stats.mannwhitneyu(maxim[:, 1, 1], maxim_control[..., 1].ravel(), alternative='greater')\n",
    "\n",
    "    # print((p1, p2))\n",
    "\n",
    "    method1 = 'FBC'\n",
    "    method2 = 'FFC'\n",
    "\n",
    "    ax[0].set_yticklabels([method1, method2], fontsize=12)\n",
    "    ax[0].set_xticks([0.0, 5, 10])\n",
    "    ax[0].set_xlim([0, 10])\n",
    "    ax[0].tick_params(axis='both', labelsize=12)\n",
    "    #ax.set_ylabel(r'$\\sum_i Im(\\lambda_i)$', fontsize=22)\n",
    "    ax[0].set_xlabel('Strength of Rotational Component above Random', fontsize=12)\n",
    "    #ax.set_title('****', fontsize=14)\n",
    "\n",
    "    ax[0].invert_yaxis()\n",
    "\n",
    "    # fill with colors\n",
    "    colors = ['red', 'black', 'blue']   \n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    whiskerprops = dict(linewidth=0)\n",
    "\n",
    "    mu = np.mean(maxim_control[..., 2], axis=1)\n",
    "    sigma = np.std(maxim_control[..., 2], axis=1)\n",
    "    r1 = maxim[:, 0, 2] - mu\n",
    "    r2 = maxim[:, 1, 2] - mu\n",
    "\n",
    "    bplot = ax[1].boxplot([r1, r2], patch_artist=True, \n",
    "                    medianprops=medianprops, notch=False, vert=False, showfliers=False, widths=[0.3, 0.3],\n",
    "                    whiskerprops=whiskerprops, showcaps=False)\n",
    "\n",
    "    x1 = maxim[:, 0, 2] - np.median(maxim_control[..., 2].ravel())\n",
    "    x2 = maxim[:, 1, 2] - np.median(maxim_control[..., 2].ravel())\n",
    "\n",
    "    _, p1 = scipy.stats.wilcoxon(x1, alternative='greater')\n",
    "    _, p2 = scipy.stats.wilcoxon(x2, alternative='greater')\n",
    "    print(p1)\n",
    "    print(p2)\n",
    "\n",
    "    method1 = 'FBC'\n",
    "    method2 = 'FFC'\n",
    "\n",
    "    ax[1].set_yticklabels([method1, method2], fontsize=12)\n",
    "    ax[1].set_xticks([0.0, 30, 60])\n",
    "    ax[1].set_xlim([0, 70])\n",
    "    ax[1].tick_params(axis='both', labelsize=12)\n",
    "    #ax.set_ylabel(r'$\\sum_i Im(\\lambda_i)$', fontsize=22)\n",
    "    ax[1].set_xlabel('Average Dynamic Range above Random', fontsize=12)\n",
    "    #ax.set_title('****', fontsize=14)\n",
    "\n",
    "    ax[1].invert_yaxis()\n",
    "\n",
    "    # fill with colors\n",
    "    colors = ['red', 'black', 'blue']   \n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "\n",
    "\n",
    "    # ax.set_xlim([13, 0])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('%s/jpca_eig_bplot_wcontrol2.pdf' % figpath, pad_inches=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
