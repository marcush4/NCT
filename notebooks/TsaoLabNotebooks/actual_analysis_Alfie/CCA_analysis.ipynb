{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as glob\n",
    "import os\n",
    "import sys; sys.path.append(\"../../..\")  # Allows access to all the scripts/modules in the larger directory\n",
    "from utils import calc_loadings\n",
    "from collections import defaultdict\n",
    "from sklearn.cross_decomposition import CCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load consolidated dimreduc dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/home/marcush/Data/TsaoLabData/neural_control_output/decoding_deg_final_analysis_230809_140453_Alfie/decoding_deg_final_analysis_230809_140453_Alfie_glom.pickle'\n",
    "path = '/home/marcush/Data/TsaoLabData/neural_control_output/testing_230322_214006_Jamie_dimreduc/testing_230322_214006_Jamie_dimreduc_glom.pickle'\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    dat = pickle.load(f) \n",
    "df_dimreduc = pd.DataFrame(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the processed data from both regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return tuple(sorted((key, make_hashable(value)) for key, value in obj.items()))\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return tuple(make_hashable(item) for item in obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "unique_hashes = set(make_hashable(d) for d in df_dimreduc['full_arg_tuple'])\n",
    "unique_dicts = [dict(u) for u in unique_hashes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('bin_width', 50), ('boxcox', 0.5), ('data_path', '/home/marcush/Data/TsaoLabData/split/degraded/degraded_only_v4_AnalysisDegraded_230322_214006_Jamie.pickle'), ('degraded', True), ('filter_fn', 'none'), ('filter_kwargs', ()), ('manual_unit_selection', (361, 379, 338, 460, 138, 542, 196, 307, 410, 512, 418, 89, 359, 491, 443, 493, 441, 230, 429, 530, 422, 310, 523, 296, 442, 417, 223, 124, 486, 546, 90, 269, 82, 120, 363, 344, 75, 519, 462, 378, 458, 91, 234, 174, 123, 199, 560, 376, 313, 286, 302, 245, 425, 244, 84, 350, 76, 327, 149, 12, 277, 211, 189, 276, 264, 477, 448, 509, 534, 203, 267, 470, 154, 209, 541, 266, 438, 449, 249, 380, 274, 92, 108, 13, 298, 31, 358, 569, 96, 253, 484, 391, 197, 252, 155, 168, 557, 437, 432, 454, 372, 433, 255, 446, 407, 236, 232, 495, 105, 399, 131, 175, 384, 440, 395, 128, 7, 32, 504, 10, 205, 23, 254, 554, 178, 387, 15, 371, 78, 565, 38, 27, 258, 343, 65, 444, 370, 294, 250, 134, 389, 471, 113, 25, 242, 285, 551, 24, 488, 172, 430, 164, 548, 180, 368, 290, 482, 382, 80, 312, 483, 538, 506, 553, 142, 19, 169, 282, 487, 18, 292, 63, 499, 328, 256, 156, 153, 567, 220, 419, 71, 177, 340, 366, 56, 555, 463, 326, 360, 46, 304, 3, 365, 59, 497, 16, 452, 53, 420, 293, 87, 110, 511, 11, 388, 524, 544, 355, 346, 30, 414, 208, 73, 102, 144, 500, 43, 283, 129, 77, 5, 41, 453, 36, 259, 52, 44, 335, 42, 159, 513, 398, 101, 556, 435, 198, 539, 436, 163, 186, 501, 97, 2, 99, 498, 218, 450, 550, 112, 392, 187, 184, 216, 206, 383, 320, 183, 61, 137, 505, 188, 247, 4, 239, 428, 162, 496, 193, 323, 427, 1, 502, 479, 70, 369, 20, 331, 257, 158, 126, 100, 173, 235, 279, 394, 140, 402, 270, 185, 0, 40, 48, 397, 9, 336, 291, 93, 401, 151, 426, 461, 165, 393, 333, 66, 381, 439, 480, 22, 106, 179, 400, 194, 299, 150, 117, 481, 176, 6, 308, 386, 233, 143, 214, 311, 494, 190, 103, 147, 68, 127, 146, 95, 406, 537, 204, 561, 305, 109, 152, 145, 132, 396, 271, 51, 268, 34, 221, 107, 45, 60, 562, 217, 364, 485, 133, 559, 227, 85, 28, 115, 552, 334, 37, 278, 114, 306, 166, 192, 33, 413, 367, 29, 289, 451, 280, 568, 339, 303, 39, 141, 98, 324, 121, 284, 314, 297, 354, 240, 434, 94, 182, 54, 478, 213, 161, 139, 191, 241, 492, 62, 88, 507, 26, 17, 57, 566, 160, 222, 111, 55, 424, 130)), ('region', 'ML'), ('spike_threshold', None), ('trial_threshold', 0))\n",
      "(('bin_width', 50), ('boxcox', 0.5), ('data_path', '/home/marcush/Data/TsaoLabData/split/degraded/degraded_only_v4_AnalysisDegraded_230322_214006_Jamie.pickle'), ('degraded', True), ('filter_fn', 'none'), ('filter_kwargs', ()), ('manual_unit_selection', (303, 50, 32, 68, 300, 60, 61, 256, 352, 284, 360, 347, 65, 369, 341, 226, 10, 54, 246, 66, 21, 156, 221, 310, 36, 207, 367, 164, 223, 79, 201, 137, 148, 127, 35, 263, 112, 210, 313, 115, 321, 111, 271, 326, 23, 42, 20, 74, 328, 192, 178, 67, 100, 272, 280, 150, 306, 27, 228, 130, 4, 73, 104, 76, 275, 187, 307, 209, 172, 142, 290, 227, 81, 200, 145, 197, 350, 98, 92, 230, 289, 232, 193, 128, 64, 229, 219, 82, 362, 252, 309, 72, 138, 124, 305, 26, 149, 250, 126, 109, 225, 291, 311, 53, 99, 159, 206, 44, 95, 247, 188, 108, 191, 237, 370, 273, 51, 203, 52, 134, 222, 94, 183, 276, 38, 327, 96, 257, 116, 55, 58, 162, 144, 211, 24, 62, 19, 118, 184, 304, 33, 214, 231, 251, 204, 84, 29, 190, 93, 146, 259, 103, 110, 69, 47, 105, 220, 80, 233, 258, 301, 180, 117, 278, 83, 315, 185, 39, 205, 264, 11, 152, 186, 46, 265, 170, 1, 113, 37, 171, 174, 312, 175, 56, 177, 292, 136, 319, 314, 107, 182, 114, 106, 48, 195, 147, 196, 363, 277, 129, 14, 70, 57, 12, 179, 224, 101, 168, 173, 194, 25, 151, 122, 318, 166, 358, 351, 260, 141, 125, 349, 161, 169, 0, 28, 157, 160, 16, 368, 18, 143, 15, 133, 288, 140, 91, 286, 43, 268, 287, 8, 266, 189, 274, 332, 249, 248, 267, 299, 242, 365, 279, 364, 366, 316, 296, 294, 17, 334, 269, 333, 297, 298, 295, 320, 348, 293, 317, 270, 261)), ('region', 'AM'), ('spike_threshold', None), ('trial_threshold', 0))\n"
     ]
    }
   ],
   "source": [
    "preload_dict_path = df_dimreduc['data_path'][0] + \"/preloaded/preloadDict.pickle\"\n",
    "\n",
    "with open(preload_dict_path, 'rb') as file:\n",
    "    preloadDict = pickle.load(file)\n",
    "\n",
    "\n",
    "for arg_dict in unique_dicts:\n",
    "    arg_tuple = tuple(sorted(arg_dict.items()))\n",
    "\n",
    "\n",
    "    for args in preloadDict.keys():\n",
    "\n",
    "        if args == arg_tuple:\n",
    "\n",
    "            preloadID = preloadDict[arg_tuple]\n",
    "            loaded_data_path = os.path.dirname(preload_dict_path) + f\"/preloaded_data_{preloadID}.pickle\"\n",
    "            \n",
    "            if arg_dict['region'] == 'AM':\n",
    "                with open(loaded_data_path, 'rb') as file:\n",
    "                    print(args)\n",
    "                    AM_loaded_data = pickle.load(file)\n",
    "\n",
    "            elif arg_dict['region'] == 'ML':\n",
    "                with open(loaded_data_path, 'rb') as file:\n",
    "                    print(args)\n",
    "                    ML_loaded_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum spike rate per trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_spikes = np.sum(AM_loaded_data['spike_rates'], 1)\n",
    "ML_spikes = np.sum(ML_loaded_data['spike_rates'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1991, 2336]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m cca_dims \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin((manual_CCA_dim, AM_spikes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], ML_spikes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      5\u001b[0m ccamodel \u001b[38;5;241m=\u001b[39m CCA(n_components\u001b[38;5;241m=\u001b[39mcca_dims)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mccamodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mML_spikes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAM_spikes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m cca_save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(df_dimreduc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_file\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/CCA_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcca_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dims.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cca_save_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/sklearn/cross_decomposition/_pls.py:233\u001b[0m, in \u001b[0;36m_PLS.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit model to data.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m        Fitted model.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    235\u001b[0m         X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy, ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    237\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    238\u001b[0m         Y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     )\n",
      "File \u001b[0;32m~/Data/anaconda3/envs/ncontrol/lib/python3.9/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1991, 2336]"
     ]
    }
   ],
   "source": [
    "manual_CCA_dim = 39 #np.inf \n",
    "\n",
    "cca_dims = np.min((manual_CCA_dim, AM_spikes.shape[-1], ML_spikes.shape[-1])).astype(int)\n",
    "\n",
    "ccamodel = CCA(n_components=cca_dims)\n",
    "ccamodel.fit(ML_spikes, AM_spikes)\n",
    "\n",
    "cca_save_path = os.path.dirname(df_dimreduc['results_file'][0]) + f\"/CCA_{cca_dims}_dims.pickle\"\n",
    "\n",
    "with open(cca_save_path, 'wb') as file:\n",
    "    pickle.dump(ccamodel, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
