{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as glob\n",
    "import os\n",
    "import sys; sys.path.append(\"../../..\")  # Allows access to all the scripts/modules in the larger directory\n",
    "from pyuoi.linear_model import UoI_L1Logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import hamming_loss\n",
    "from collections import defaultdict\n",
    "from utils import calc_loadings\n",
    "from scipy.stats import pearsonr\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load consolidated decoding dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_glom_path = '/home/marcush/Data/TsaoLabData/neural_control_output/degraded_decoding_param_search/degraded_decoding_param_search_glom.pickle'\n",
    "with open(decoding_glom_path, 'rb') as f:\n",
    "    dat_decode = pickle.load(f) \n",
    "\n",
    "df_decode = pd.DataFrame(dat_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preloaded_data_path = glob.glob(df_decode['data_path'][0] + \"/preloaded/preloaded_data_*.pickle\")[0]\n",
    "with open(preloaded_data_path, 'rb') as f:\n",
    "    preload_dat = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashable(d):\n",
    "    return tuple(sorted((key, make_hashable(value)) if isinstance(value, dict) else (key, value)\n",
    "                        for key, value in d.items()))\n",
    "\n",
    "\n",
    "unique_hashes = set(make_hashable(d) for d in df_decode['loader_args'])\n",
    "unique_dicts = [dict(u) for u in unique_hashes]\n",
    "\n",
    "for u in unique_dicts:\n",
    "    u['data_path'] = df_decode['data_path'][0] + \"/\" + df_decode['data_file'][0]\n",
    "    u['spike_threshold'] = None\n",
    "    u['trial_threshold'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_dict_path = df_decode['data_path'][0] + \"/preloaded/preloadDict.pickle\"\n",
    "\n",
    "with open(preload_dict_path, 'rb') as file:\n",
    "    preloadDict = pickle.load(file)\n",
    "\n",
    "\n",
    "for arg_dict in unique_dicts:\n",
    "    arg_tuple = tuple(sorted(arg_dict.items()))\n",
    "\n",
    "\n",
    "    for args in preloadDict.keys():\n",
    "\n",
    "        if args == arg_tuple:\n",
    "\n",
    "            preloadID = preloadDict[arg_tuple]\n",
    "            loaded_data_path = os.path.dirname(preload_dict_path) + f\"/preloaded_data_{preloadID}.pickle\"\n",
    "            \n",
    "            if arg_dict['region'] == 'AM':\n",
    "                with open(loaded_data_path, 'rb') as file:\n",
    "                    AM_loaded_data = pickle.load(file)\n",
    "\n",
    "            elif arg_dict['region'] == 'ML':\n",
    "                with open(loaded_data_path, 'rb') as file:\n",
    "                    ML_loaded_data = pickle.load(file)\n",
    "\n",
    "AM_spikes = np.sum(AM_loaded_data['spike_rates'], 1)\n",
    "ML_spikes = np.sum(ML_loaded_data['spike_rates'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degradedIDs = preload_dat['degradedIDs']\n",
    "dimensions = np.unique(df_decode['dim'])\n",
    "n_folds = np.unique(df_decode['fold_idx'])\n",
    "regions = np.unique(df_decode['loader_args'].apply(lambda x: x.get('region')))\n",
    "dimreduc_methods = np.unique(df_decode['dimreduc_method'])\n",
    "stimIDs = AM_loaded_data['StimIDs']\n",
    "degraded_trial_IDs = AM_loaded_data['stratifiedIDs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_dat.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding if a trial is degraded vs clear (using sklearn's logistic regression - default L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10\n",
    "kf = KFold(num_splits)\n",
    "AM_weights_degclear = np.zeros((num_splits, AM_spikes.shape[1]))\n",
    "losses = np.zeros(num_splits)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(AM_spikes)):\n",
    "\n",
    "    Xtrain = AM_spikes[train_index,:]\n",
    "    Ytrain = degraded_trial_IDs[train_index]\n",
    "\n",
    "    Xtest = AM_spikes[test_index,:]\n",
    "    Ytest = degraded_trial_IDs[test_index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtrain = scaler.fit_transform(Xtrain)\n",
    "\n",
    "    clf = LogisticRegression().fit(Xtrain, Ytrain)\n",
    "    predictions = clf.predict(scaler.fit_transform(Xtest))\n",
    "    loss = hamming_loss(Ytest, predictions)\n",
    "    losses[i] = loss\n",
    "\n",
    "    AM_weights_degclear[i, :] = np.mean(np.abs(clf.coef_), 0)\n",
    "\n",
    "AM_weights_degclear = np.mean(AM_weights_degclear, 0)\n",
    "print(f\"Average loss for region AM on degraded vs clear: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10\n",
    "kf = KFold(num_splits)\n",
    "ML_weights_degclear = np.zeros((num_splits, ML_spikes.shape[1]))\n",
    "losses = np.zeros(num_splits)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(ML_spikes)):\n",
    "\n",
    "    Xtrain = ML_spikes[train_index,:]\n",
    "    Ytrain = degraded_trial_IDs[train_index]\n",
    "\n",
    "    Xtest = ML_spikes[test_index,:]\n",
    "    Ytest = degraded_trial_IDs[test_index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtrain = scaler.fit_transform(Xtrain)\n",
    "\n",
    "    clf = LogisticRegression().fit(Xtrain, Ytrain)\n",
    "    predictions = clf.predict(scaler.fit_transform(Xtest))\n",
    "    loss = hamming_loss(Ytest, predictions)\n",
    "    losses[i] = loss\n",
    "\n",
    "    ML_weights_degclear[i, :] = np.mean(np.abs(clf.coef_), 0)\n",
    "\n",
    "ML_weights_degclear = np.mean(ML_weights_degclear, 0)\n",
    "print(f\"Average loss for region ML on degraded vs clear: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding stim ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10\n",
    "kf = KFold(num_splits)\n",
    "AM_weights_stimID = np.zeros((num_splits, AM_spikes.shape[1]))\n",
    "losses = np.zeros(num_splits)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(AM_spikes)):\n",
    "\n",
    "    Xtrain = AM_spikes[train_index,:]\n",
    "    Ytrain = stimIDs[train_index]\n",
    "\n",
    "    Xtest = AM_spikes[test_index,:]\n",
    "    Ytest = stimIDs[test_index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtrain = scaler.fit_transform(Xtrain)\n",
    "\n",
    "    clf = LogisticRegression(multi_class=\"multinomial\").fit(Xtrain, Ytrain)\n",
    "    predictions = clf.predict(scaler.fit_transform(Xtest))\n",
    "    loss = hamming_loss(Ytest, predictions)\n",
    "    losses[i] = loss\n",
    "\n",
    "    AM_weights_stimID[i, :] = np.mean(np.abs(clf.coef_), 0)\n",
    "\n",
    "AM_weights_stimID = np.mean(AM_weights_stimID, 0)\n",
    "print(f\"Average loss for region ML on Stim IDs: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 10\n",
    "kf = KFold(num_splits)\n",
    "ML_weights_stimID = np.zeros((num_splits, ML_spikes.shape[1]))\n",
    "losses = np.zeros(num_splits)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(ML_spikes)):\n",
    "\n",
    "    Xtrain = ML_spikes[train_index,:]\n",
    "    Ytrain = stimIDs[train_index]\n",
    "\n",
    "    Xtest = ML_spikes[test_index,:]\n",
    "    Ytest = stimIDs[test_index]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    Xtrain = scaler.fit_transform(Xtrain)\n",
    "\n",
    "    clf = LogisticRegression(multi_class=\"multinomial\").fit(Xtrain, Ytrain)\n",
    "    predictions = clf.predict(scaler.fit_transform(Xtest))\n",
    "    loss = hamming_loss(Ytest, predictions)\n",
    "    losses[i] = loss\n",
    "\n",
    "    ML_weights_stimID[i, :] = np.mean(np.abs(clf.coef_), 0)\n",
    "\n",
    "ML_weights_stimID = np.mean(ML_weights_stimID, 0)\n",
    "print(f\"Average loss for region ML on Stim IDs: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing importance scores to logistic regression weights for binary classificaiton (degraded vs clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccamodel_dir = df_decode['results_file'][0]\n",
    "CCA_dims = 25 #None\n",
    "\n",
    "if CCA_dims == None:\n",
    "    ccamodel_path = glob.glob(ccamodel_dir+\"/CCA_*.pickle\")[0]\n",
    "else:\n",
    "    ccamodel_path = os.path.dirname(ccamodel_dir)+f\"/CCA_{CCA_dims}_dims.pickle\"\n",
    "\n",
    "with open(ccamodel_path, 'rb') as file:\n",
    "    ccamodel = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_defaultdict():\n",
    "    return defaultdict(recursive_defaultdict)\n",
    "importance_scores = recursive_defaultdict()\n",
    "\n",
    "\n",
    "\n",
    "for reg in regions:\n",
    "    if reg == 'ML':\n",
    "        cca_proj = ccamodel.x_rotations_\n",
    "    else:\n",
    "        cca_proj = ccamodel.y_rotations_\n",
    "\n",
    "    for dim in dimensions:\n",
    "\n",
    "        importance_scores[reg]['CCA'][dim] = calc_loadings(cca_proj[:, 0:dim])\n",
    "        \n",
    "        for method in dimreduc_methods:    \n",
    "            all_scores = np.zeros((len(n_folds), cca_proj.shape[0]))\n",
    "\n",
    "            for n_fold in n_folds:\n",
    "\n",
    "                coef = df_decode[(df_decode['loader_args'].apply(lambda x: x.get('region')) == reg)  & \n",
    "                                            (df_decode['dim'] == dim) & (df_decode['fold_idx'] == n_fold) &\n",
    "                                            (df_decode['dimreduc_method'] == method)]['coef'].iloc[0]\n",
    "                \n",
    "                all_scores[n_fold, :] = calc_loadings(coef)\n",
    "\n",
    "            importance_scores[reg][method][dim] = np.mean(all_scores, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'AM'\n",
    "DIM = 39\n",
    "FFC_importance = importance_scores[region]['PCA'][DIM]\n",
    "FBC_importance = importance_scores[region]['LQGCA'][DIM]\n",
    "CCA_importance = importance_scores[region]['CCA'][DIM]\n",
    "reg_importance = AM_weights_degclear\n",
    "\n",
    "RegVFFC_corr, p_value = pearsonr(reg_importance, FFC_importance)\n",
    "RegVFBC_corr, p_value = pearsonr(reg_importance, FBC_importance)\n",
    "RegVCCA_corr, p_value = pearsonr(reg_importance, CCA_importance)\n",
    "\n",
    "\n",
    "plt.scatter(reg_importance, FFC_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: FFC vs Regression Correlation: {np.round(RegVFFC_corr, 3)}')\n",
    "plt.xlabel('Logistic Regression Coefficients (Binary, Degraded v Clear)')  \n",
    "plt.ylabel(f'FFC Importance Scores')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(reg_importance, FBC_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: FBC vs Regression Correlation: {np.round(RegVFBC_corr, 3)}')\n",
    "plt.xlabel('Logistic Regression Coefficients (Binary, Degraded v Clear)')  \n",
    "plt.ylabel(f'FBC Importance Scores')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(reg_importance, CCA_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: CCA vs Regression Correlation: {np.round(RegVCCA_corr, 3)}')\n",
    "plt.xlabel('Logistic Regression Coefficients (Binary, Degraded v Clear)')  \n",
    "plt.ylabel(f'CCA Importance Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'ML'\n",
    "DIM = 21\n",
    "FFC_importance = importance_scores[region]['PCA'][DIM]\n",
    "FBC_importance = importance_scores[region]['LQGCA'][DIM]\n",
    "CCA_importance = importance_scores[region]['CCA'][DIM]\n",
    "reg_importance = ML_weights_degclear\n",
    "\n",
    "RegVFFC_corr, p_value = pearsonr(np.squeeze(reg_importance), FFC_importance)\n",
    "RegVFBC_corr, p_value = pearsonr(np.squeeze(reg_importance), FBC_importance)\n",
    "RegVCCA_corr, p_value = pearsonr(np.squeeze(reg_importance), CCA_importance)\n",
    "\n",
    "\n",
    "plt.scatter(reg_importance, FFC_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: FFC vs Regression Correlation: {np.round(RegVFFC_corr, 3)}')\n",
    "plt.xlabel('Logistic Regression Coefficients (Binary, Degraded v Clear)')  \n",
    "plt.ylabel(f'FFC Importance Scores')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(reg_importance, FBC_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: FBC vs Regression Correlation: {np.round(RegVFBC_corr, 3)}')\n",
    "plt.xlabel('Logistic Regression Coefficients (Binary, Degraded v Clear)')  \n",
    "plt.ylabel(f'FBC Importance Scores')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(reg_importance, CCA_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: CCA vs Regression Correlation: {np.round(RegVCCA_corr, 3)}')\n",
    "plt.xlabel('Logistic Regression Coefficients (Binary, Degraded v Clear)')  \n",
    "plt.ylabel(f'CCA Importance Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing importance scores to logistic regression weights for classificaiton on stimIDs using sklearn's LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'AM'\n",
    "DIM = 39\n",
    "FFC_importance = importance_scores[region]['PCA'][DIM]\n",
    "FBC_importance = importance_scores[region]['LQGCA'][DIM]\n",
    "CCA_importance = importance_scores[region]['CCA'][DIM]\n",
    "\n",
    "\n",
    "RegVFFC_corr, p_value = pearsonr(AM_weights_stimID, FFC_importance)\n",
    "RegVFBC_corr, p_value = pearsonr(AM_weights_stimID, FBC_importance)\n",
    "RegVCCA_corr, p_value = pearsonr(AM_weights_stimID, CCA_importance)\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(AM_weights_stimID, FFC_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: FFC vs Regression Correlation: {np.round(RegVFFC_corr, 3)}')\n",
    "plt.xlabel('LogReg Regression Coefficients (Multinomial, StimIDs)')  \n",
    "plt.ylabel(f'FFC Importance Scores')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(AM_weights_stimID, FBC_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: FBC vs Regression Correlation: {np.round(RegVFBC_corr, 3)}')\n",
    "plt.xlabel('LogReg Regression Coefficients (Multinomial, StimIDs)')  \n",
    "plt.ylabel(f'FBC Importance Scores')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(AM_weights_stimID, CCA_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: CCA vs Regression Correlation: {np.round(RegVCCA_corr, 3)}')\n",
    "plt.xlabel('LogReg Regression Coefficients (Multinomial, StimIDs)')  \n",
    "plt.ylabel(f'CCA Importance Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'ML'\n",
    "DIM = 21\n",
    "FFC_importance = importance_scores[region]['PCA'][DIM]\n",
    "FBC_importance = importance_scores[region]['LQGCA'][DIM]\n",
    "CCA_importance = importance_scores[region]['CCA'][DIM]\n",
    "\n",
    "\n",
    "RegVFFC_corr, p_value = pearsonr(ML_weights_stimID, FFC_importance)\n",
    "RegVFBC_corr, p_value = pearsonr(ML_weights_stimID, FBC_importance)\n",
    "RegVCCA_corr, p_value = pearsonr(ML_weights_stimID, CCA_importance)\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(ML_weights_stimID, FFC_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: FFC vs Regression Correlation: {np.round(RegVFFC_corr, 3)}')\n",
    "plt.xlabel('LogReg Regression Coefficients (Multinomial, StimIDs)')  \n",
    "plt.ylabel(f'FFC Importance Scores')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(ML_weights_stimID, FBC_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: FBC vs Regression Correlation: {np.round(RegbVFBC_corr, 3)}')\n",
    "plt.xlabel('LogReg Regression Coefficients (Multinomial, StimIDs)')  \n",
    "plt.ylabel(f'FBC Importance Scores')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(ML_weights_stimID, CCA_importance, marker='x', s=2, color='r')\n",
    "plt.title(f'Region {region}: CCA vs Regression Correlation: {np.round(RegVCCA_corr, 3)}')\n",
    "plt.xlabel('LogReg Regression Coefficients (Multinomial, StimIDs)')  \n",
    "plt.ylabel(f'CCA Importance Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting correlations against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AM_binary = [0.209, -0.018, 0.299]\n",
    "AM_multiclass = [-0.08, -0.083, 0.048]\n",
    "\n",
    "ML_binary = [0.123, -0.187, 0.194]\n",
    "ML_multiclass = [0.269, -0.43, 0.282]\n",
    "\n",
    "corrs = np.zeros((3, 4))\n",
    "corrs[:, 0] = AM_binary\n",
    "corrs[:, 1] = AM_multiclass\n",
    "corrs[:, 2] = ML_binary\n",
    "corrs[:, 3] = ML_multiclass\n",
    "\n",
    "x =  corrs[0,:]\n",
    "y = corrs[1,:]\n",
    "z = corrs[2,:]\n",
    "\n",
    "colors = [[1,0,0, 1],[1,0,0, 0.6],[0,0,1, 1],[0,0,1, 0.6]]\n",
    "labels = [\"AM binary\", \"AM multiclass\", \"ML binary\", \"ML multiclass\"]\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "fig = go.Figure(data=[go.Scatter3d( x=x, y=y, z=z, mode='markers', marker=dict( size=5, color=colors, opacity=0.8 ), text=labels, textposition='top center'  )])\n",
    "\n",
    "axis_limits = [-0.5,0.5]\n",
    "fig.update_layout( scene=dict( xaxis=dict(title='FFC vs Reg Weight', range=axis_limits, zeroline=True, zerolinewidth=2, zerolinecolor='black', showline=True, showgrid=True, gridcolor='lightgray'), \n",
    "                              yaxis=dict(title='FBC vs Reg Weight',  range=axis_limits, zeroline=True, zerolinewidth=2, zerolinecolor='black', showline=True, showgrid=True, gridcolor='lightgray'),  \n",
    "                              zaxis=dict(title='CCA vs Reg Weight', range=axis_limits, zeroline=True, zerolinewidth=2, zerolinecolor='black', showline=True, showgrid=True, gridcolor='lightgray'),\n",
    "                              camera=dict(eye=dict(x=1.25, y=1.25, z=1.25)) ), title=f'Region {region} subspace importance scores vs regression weight', height=500 )\n",
    "\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"ML_importance_scores.html\")\n",
    "#iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tried using Union of Intersections (UoI) decoding, but the code base is pretty outdated/doesn't perform that well\n",
    "### The idea was that we have data from two different areas that have different levels of face selectivity, different number of neurons, and other various uncontrolled parameters. Thus, how are we to compare differences in their decoding accuracy given these biases? The idea was that UoI regression would factor in these biases and perform the best-case-scenario unbiased sestimate of accuracy.\n",
    "### Unfortunately, for binary decoding (where we have a serious, 90/10 imbalance of degraded vs clear trial types) it just outputs that all trials are degraded, and for multiclass classification (stimulus ID) the code simply does not run/returns syntax errors from deep in the code\n",
    "### Here is the code that I had been using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uoi_ML_save_path = os.path.dirname(df_decode['results_file'][0]) + f\"/UOI_ML.pickle\"\n",
    "with open(uoi_ML_save_path, 'rb') as file:\n",
    "    uoi_ML = pickle.load(file)\n",
    "\n",
    "\n",
    "uoi_AM_save_path = os.path.dirname(df_decode['results_file'][0]) + f\"/UOI_AM.pickle\"\n",
    "with open(uoi_AM_save_path, 'rb') as file:\n",
    "    uoi_AM = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = AM_spikes\n",
    "y = degraded_trial_IDs\n",
    "indices = np.arange(len(X))\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, indices, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "\n",
    "fitter = LogisticRegression().fit(X_train, y_train)\n",
    "support = np.ones(X.shape[1]).astype(bool)\n",
    "\n",
    "\n",
    "uoi_AM = UoI_L1Logistic(estimation_score='BIC')\n",
    "assert uoi_AM._estimation_target == 0\n",
    "uoi_AM.classes_ = np.unique(y)\n",
    "score = -1 * uoi_AM._score_predictions('BIC', fitter, X, y, support, (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(uoi_AM.predict_proba(AM_spikes[test_indices, :]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = AM_spikes\n",
    "y = degraded_trial_IDs\n",
    "indices = np.arange(len(X))\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, indices, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "uoi_AM = UoI_L1Logistic(estimation_score='BIC', estimation_target='train', random_state=10).fit(AM_spikes[train_indices, :], degraded_trial_IDs[train_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uoi_AM = UoI_L1Logistic(estimation_score='BIC', estimation_target='train', random_state=10).fit(AM_spikes, degraded_trial_IDs)\n",
    "uoi_ML = UoI_L1Logistic(estimation_score='BIC',  estimation_target='train', random_state=10).fit(ML_spikes, degraded_trial_IDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uoi_ML_save_path = os.path.dirname(df_decode['results_file'][0]) + f\"/UOI_ML.pickle\"\n",
    "with open(uoi_ML_save_path, 'wb') as file:\n",
    "    pickle.dump(uoi_ML, file)\n",
    "\n",
    "\n",
    "uoi_AM_save_path = os.path.dirname(df_decode['results_file'][0]) + f\"/UOI_AM.pickle\"\n",
    "with open(uoi_AM_save_path, 'wb') as file:\n",
    "    pickle.dump(uoi_AM, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uoi_AM.score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncontrol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
