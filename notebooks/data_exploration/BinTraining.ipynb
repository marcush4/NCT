{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = {'indy_20160426_01': 0,\n",
    "               'indy_20160622_01':1700,\n",
    "               'indy_20160624_03': 500,\n",
    "               'indy_20160627_01': 0,\n",
    "               'indy_20160630_01': 0,\n",
    "               'indy_20160915_01': 0,\n",
    "               'indy_20160921_01': 0,\n",
    "               'indy_20160930_02': 0,\n",
    "               'indy_20160930_05': 300,\n",
    "               'indy_20161005_06': 0,\n",
    "               'indy_20161006_02': 350,\n",
    "               'indy_20161007_02': 950,\n",
    "               'indy_20161011_03': 0,\n",
    "               'indy_20161013_03': 0,\n",
    "               'indy_20161014_04': 0,\n",
    "               'indy_20161017_02': 0,\n",
    "               'indy_20161024_03': 0,\n",
    "               'indy_20161025_04': 0,\n",
    "               'indy_20161026_03': 0,\n",
    "               'indy_20161027_03': 500,\n",
    "               'indy_20161206_02': 5500,\n",
    "               'indy_20161207_02': 0,\n",
    "               'indy_20161212_02': 0,\n",
    "               'indy_20161220_02': 0,\n",
    "               'indy_20170123_02': 0,\n",
    "               'indy_20170124_01': 0,\n",
    "               'indy_20170127_03': 0,\n",
    "               'indy_20170131_02': 0,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loadings(U, d=1):\n",
    "    # Sum over components\n",
    "    U = np.sum(np.power(np.abs(U), 2), axis=-1)\n",
    "    # Reshape and then sum over neurons\n",
    "    U = np.reshape(U, (d, -1))\n",
    "    loadings = np.sum(U, axis=0)\n",
    "    loadings /= np.max(loadings)\n",
    "    return loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pdb\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from loaders import load_sabes \n",
    "from utils import apply_df_filters\n",
    "from decoders import lr_decoder\n",
    "from segmentation import reach_segment_sabes\n",
    "#from dca.dca import DynamicalComponentsAnalysis\n",
    "from dca_research.kca import KalmanComponentsAnalysis as KCA\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_decoding_files = glob.glob('/media/akumar/Secondary/data/cosyne_results/sabes_decoding/sabes_decoding_*.dat')\n",
    "sabes_kca_files = glob.glob('/media/akumar/Secondary/data/cosyne_results/sabes_kca/sabes_kca_*.dat')\n",
    "onlyfiles = [f for f in listdir(\"/media/akumar/Secondary/data/sabes\") \n",
    "             if isfile(join(\"/media/akumar/Secondary/data/sabes\", f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for file in sabes_kca_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result_ = pickle.load(f)\n",
    "    results_list.extend(result_)\n",
    "sabes_kca_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_DCAnPCAonBin(X):\n",
    "    \"\"\"Now just used to calculate PCA and DCA loading\"\"\"\n",
    "    KCAmodel = KCA(d=2, T=3, causal_weights=(0, 1), project_mmse=True)\n",
    "    PCAmodel = PCA(n_components=2)\n",
    "    KCAmodel.fit(X)\n",
    "    extended = X[0]\n",
    "    for transit in X[1:]:\n",
    "        extended = np.vstack((extended,transit))\n",
    "    PCAmodel.fit(extended)\n",
    "    PCA_loading = calc_loadings(PCAmodel.components_.T)\n",
    "    KCA_loading = calc_loadings(KCAmodel.coef_)\n",
    "    return PCA_loading, KCA_loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sprVSRsqNsqaured_sum_coef(rsquared, coefs,loading):\n",
    "    \"\"\"Given each neuron's loading.\n",
    "        Get the spearmanr of loading V.S rsqaured of this neruon in the binned model,\n",
    "        as well as spearmanr of loading V.S squared sum of sin/cos coefficients of this neruon in the model.\n",
    "        Return correlations\"\"\"\n",
    "    spearmanr_loading_rsquared = spearmanr(loading, rsquared)\n",
    "    sqaured_sum_coef = np.add(np.square(coefs[:,0]),np.square(coefs[:,1]))\n",
    "    spearmanr_loading_sqauredsum_coef = spearmanr(loading,sqaured_sum_coef)\n",
    "    return spearmanr_loading_rsquared.correlation,spearmanr_loading_sqauredsum_coef.correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D(t - \\tau) - b_0 = \\lVert V(t) \\lVert (b_n + b_y sin[\\theta (t)] + b_x cos[\\theta(t)])$\n",
    "### we can fit the following linear model\n",
    "$D(t - \\tau) = b_0 + b_n \\lVert V(t) \\lVert + b_y \\lVert V(t) \\lVert sin[\\theta (t)] + b_x \\lVert V(t) \\lVert cos[\\theta(t)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dim', 'fold_idx', 'train_idxs', 'test_idxs', 'KCA', 'fit_all',\n",
       "       'bin_width', 'filter_fn', 'filter_kwargs', 'boxcox', 'spike_threshold',\n",
       "       'dim_vals', 'n_folds', 'T', 'ols_order', 'data_file', 'PCA', 'SFA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sabes_kca_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [08:50, 18.96s/it]\n"
     ]
    }
   ],
   "source": [
    "tau = [0, 2, 4, 6, 8]\n",
    "data_path = '/media/akumar/Secondary/data/sabes'\n",
    "decoder_params = {'trainlag': 4, 'testlag': 4, 'decoding_window': 3}\n",
    "bins = np.arange(-np.pi,np.pi,.25 * np.pi)\n",
    "times_binning_better = 0\n",
    "data_files = np.unique(sabes_kca_df['data_file'].values)\n",
    "results_list = []\n",
    "\n",
    "for i, file in tqdm(enumerate(data_files)):\n",
    "    KCA_df = apply_df_filters(sabes_kca_df, data_file=file, fold_idx=1, dim=1)    \n",
    "    dat = load_sabes('%s/%s' % (data_path, file), bin_width=KCA_df.iloc[0][\"bin_width\"],\n",
    "                     filter_fn=KCA_df.iloc[0]['filter_fn'], filter_kwargs=KCA_df.iloc[0]['filter_kwargs'],\n",
    "                     boxcox=KCA_df.iloc[0]['boxcox'], spike_threshold=KCA_df.iloc[0]['spike_threshold'])\n",
    "    \n",
    "    dat_segmented = reach_segment_sabes(dat, start_times[file.split(\".\")[0]])\n",
    "    spike_rates = dat_segmented['spike_rates']\n",
    "    spike_rates = spike_rates.reshape(spike_rates.shape[1], -1)\n",
    "    vels = dat_segmented['vel']\n",
    "    #‖𝑉(𝑡)‖\n",
    "    peak_vels_in_windows = np.array([np.amax(np.absolute(vels[start : end + 1])) \\\n",
    "                                    for start, end in dat_segmented['transition_times']])[:,np.newaxis]\n",
    "    orientation_in_windows = dat_segmented['transition_orientation']\n",
    "\n",
    "    #‖𝑉(𝑡)‖𝑠𝑖𝑛[𝜃(𝑡)]\n",
    "    peak_vels_in_windows = normalize(peak_vels_in_windows, axis = 0)\n",
    "    vel_sin = normalize(np.sin(orientation_in_windows)[:,np.newaxis] * peak_vels_in_windows, axis = 0)\n",
    "#     vel_sin = np.sin(orientation_in_windows)[:,np.newaxis] * peak_vels_in_windows\n",
    "    #‖𝑉(𝑡)‖𝑐𝑜𝑠[𝜃(𝑡)]\n",
    "    vel_cos = np.cos(orientation_in_windows)[:,np.newaxis] * peak_vels_in_windows\n",
    "#    vel_cos = normalize(np.cos(orientation_in_windows)[:,np.newaxis] * peak_vels_in_windows, axis = 0)\n",
    "\n",
    "    #Binning\n",
    "    binned_indices = np.digitize(orientation_in_windows, bins)\n",
    "    binned_indices = [np.where(binned_indices == idx) for idx in range(1,9)]\n",
    "\n",
    "    #To record the r^2 and coeffs indexed by (bin#, neuron#)\n",
    "    r_squared_bin_neuron = np.zeros((len(binned_indices), len(tau), spike_rates.shape[1]))\n",
    "    coefficients_bin_neuron = np.zeros((len(binned_indices), len(tau), spike_rates.shape[1], 2))\n",
    "\n",
    "    for j in range(8):\n",
    "        binned_idx = binned_indices[j]\n",
    "        transitions_inbin = np.array(dat_segmented['transition_times'])[binned_idx]\n",
    "        # Get the input as ‖𝑉(𝑡)‖, ‖𝑉(𝑡)‖𝑠𝑖𝑛[𝜃(𝑡)], ‖𝑉(𝑡)‖𝑐𝑜𝑠[𝜃(𝑡)]\n",
    "        X = np.concatenate((peak_vels_in_windows[binned_idx], \\\n",
    "                            vel_sin[binned_idx], vel_cos[binned_idx]), axis = 1)\n",
    "        for k, t_ in enumerate(tau):\n",
    "            for neuron_idx in range(spike_rates.shape[1]):\n",
    "                spike_rates_neuron = spike_rates[:,neuron_idx]\n",
    "                average_rates_in_windows = np.array([np.average(spike_rates_neuron[max(start_time - t_, 0): \\\n",
    "                                                                                   min(end_time + 1 - t_, spike_rates.shape[0])])\n",
    "                                                     for start_time, end_time in transitions_inbin])            \n",
    "                #Train the model - features are normalized\n",
    "    #             X = StandardScaler().fit_transform(X)\n",
    "    #             average_rates_in_windows = StandardScaler().fit_transform(average_rates_in_windows.reshape(-1, 1))\n",
    "                reg = LinearRegression().fit(X, average_rates_in_windows)\n",
    "                #Getting r sqaured, and put it in the recording array\n",
    "                r_squared_bin_neuron[j, k, neuron_idx] = reg.score(X, average_rates_in_windows)\n",
    "                #Do the same for coefficients for ‖𝑉(𝑡)‖𝑠𝑖𝑛[𝜃(𝑡)], ‖𝑉(𝑡)‖𝑐𝑜𝑠[𝜃(𝑡)]\n",
    "                coefficients_bin_neuron[j, k, neuron_idx] = np.squeeze(reg.coef_)[1:]\n",
    "\n",
    "            #Prepare to fit DCA with the bin\n",
    "            expanded_transition_times = [list(range(max(0, start - t_), min(end+1 - t_, spike_rates.shape[0]))) \n",
    "                                         for start,end in transitions_inbin]\n",
    "            spike_rates_list_transition = [spike_rates[transit] for transit in expanded_transition_times]\n",
    "            PCA_loading, KCA_loading = fit_DCAnPCAonBin(spike_rates_list_transition)\n",
    "            #Getting the spearmanr correlation for r^2 and sqaured sum of coefficients\n",
    "            spr_r2_dca, spr_coef_dca = sprVSRsqNsqaured_sum_coef(r_squared_bin_neuron[j, k], \\\n",
    "                                                                 coefficients_bin_neuron[j, k],KCA_loading)\n",
    "            spr_r2_pca, spr_coef_pca = sprVSRsqNsqaured_sum_coef(r_squared_bin_neuron[j, k], \\\n",
    "                                                                 coefficients_bin_neuron[j, k],PCA_loading)\n",
    "        \n",
    "            # Now do so for only the top 20% most tuned neurons\n",
    "            n20 = int(0.2 * spike_rates.shape[1])\n",
    "            top_r2_neurons = np.argsort(r_squared_bin_neuron[j, k])[::-1][0:n20]\n",
    "            top_coef_neurons = np.argsort(np.linalg.norm(coefficients_bin_neuron[j, k]))[::-1][0:n20]\n",
    "\n",
    "            spr_r2_dca_tr2, spr_coef_dca_tr2 = sprVSRsqNsqaured_sum_coef(r_squared_bin_neuron[j, k, top_r2_neurons],\n",
    "                                                                         coefficients_bin_neuron[j, k, top_r2_neurons],\n",
    "                                                                         KCA_loading[top_r2_neurons])\n",
    "\n",
    "            spr_r2_pca_tr2, spr_coef_pca_tr2 = sprVSRsqNsqaured_sum_coef(r_squared_bin_neuron[j, k, top_r2_neurons], \\\n",
    "                                                                         coefficients_bin_neuron[j, k, top_r2_neurons],\n",
    "                                                                         PCA_loading[top_r2_neurons])\n",
    "            \n",
    "            spr_r2_dca_tc, spr_coef_dca_tc = sprVSRsqNsqaured_sum_coef(r_squared_bin_neuron[j, k, top_coef_neurons], \\\n",
    "                                                                       coefficients_bin_neuron[j, k, top_coef_neurons],\n",
    "                                                                       KCA_loading[top_coef_neurons])\n",
    "            \n",
    "            spr_r2_pca_tc, spr_coef_pca_tc = sprVSRsqNsqaured_sum_coef(r_squared_bin_neuron[j, k, top_coef_neurons], \\\n",
    "                                                                       coefficients_bin_neuron[j, k, top_coef_neurons],\n",
    "                                                                       PCA_loading[top_coef_neurons])\n",
    "\n",
    "            \n",
    "            \n",
    "            # Append results\n",
    "            result = {'file': file, 'bin_idx':j, 'tau': t_, \n",
    "                      'tuning_r2':r_squared_bin_neuron, 'theta_coef':coefficients_bin_neuron,\n",
    "                      'PCA_loadings':PCA_loading, 'KCA_loading':KCA_loading,\n",
    "                      'spr_r2_dca':spr_r2_dca, 'spr_coef_dca': spr_coef_dca,\n",
    "                      'spr_r2_pca':spr_r2_pca, 'spr_coef_pca': spr_coef_pca,\n",
    "                      # Top r2 neurons\n",
    "                      'spr_r2_dca_tr2':spr_r2_dca_tr2, 'spr_coef_dca_tr2': spr_coef_dca_tr2,\n",
    "                      # Top coef neurons\n",
    "                      'spr_r2_dca_tc':spr_r2_dca_tc, 'spr_coef_dca_tc': spr_coef_dca_tc,\n",
    "                      'spr_r2_pca_tr2':spr_r2_pca_tr2, 'spr_coef_pca_tr2': spr_coef_pca_tr2,\n",
    "                      'spr_r2_pca_tc':spr_r2_pca_tc, 'spr_coef_pca_tc': spr_coef_pca_tc}\n",
    "                      \n",
    "                      \n",
    "            results_list.append(result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('orientation_tuning_df.dat', 'wb') as f:\n",
    "#     f.write(pickle.dumps(result_df))\n",
    "with open('/home/akumar/nse/neural_control/analysis_scripts/orientation_df.dat', 'rb') as f:\n",
    "    result_df = pickle.load(f)\n",
    "with open('/home/akumar/nse/neural_control/analysis_scripts/orientation_df_unnorm.dat', 'rb') as f:\n",
    "    result_df2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ",# Tau is a hyperparameter \n",
    "# Also segment by additional hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "with open('v3/sabes_tuning_correlation.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(cor_bins_coef_dca))\n",
    "    f.write(pickle.dumps(cor_bins_coef_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_4144/981099321.py\u001b[0m(42)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     40 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     41 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 42 \u001b[0;31m            \u001b[0mrkca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspearmanr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KCA_loading'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     44 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** NameError: name 'df1' is not defined\n",
      "(186,)\n",
      "file                                       indy_20160426_01.mat\n",
      "bin_idx                                                       0\n",
      "tau                                                           0\n",
      "tuning_r2     [[[0.16162009651905296, 0.07186219296061436, 0...\n",
      "theta_coef    [[[[0.16225998 0.2369658 ], [0.06874804 0.0497...\n",
      "Name: 0, dtype: object\n",
      "(8, 5, 186)\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4144/981099321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mrkca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspearmanr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KCA_loading'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4144/981099321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mrkca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspearmanr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KCA_loading'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dyn/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dyn/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def box_plot(ax, data, edge_color, fill_color):\n",
    "    bp = ax.boxplot([data[i, :] for i in range(data.shape[0])], patch_artist=True)\n",
    "    \n",
    "    for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "        plt.setp(bp[element], color=edge_color)\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=fill_color)       \n",
    "        \n",
    "    return bp\n",
    "    \n",
    "tau = [0, 2, 4, 6, 8]\n",
    "\n",
    "shape = ((8, len(tau), 28))\n",
    "    \n",
    "cor_bins_coef_dca = np.zeros(shape)\n",
    "cor_bins_r2_dca = np.zeros(shape)\n",
    "cor_bins_coef_pca = np.zeros(shape)\n",
    "cor_bins_r2_pca = np.zeros(shape)\n",
    "\n",
    "cor_bins_dca_tc = np.zeros(shape)\n",
    "cor_bins_dca_tr2 = np.zeros(shape)\n",
    "cor_bins_pca_tc = np.zeros(shape)\n",
    "cor_bins_pca_tr2 = np.zeros(shape)\n",
    "\n",
    "for bin_idx in range(8):\n",
    "    for tidx, t_ in enumerate(tau):\n",
    "        df_ = result_df.loc[result_df['bin_idx'] == bin_idx].loc[result_df['tau'] == t_]\n",
    "        df_2 = result_df2.loc[result_df2['bin_idx'] == bin_idx].loc[result_df2['tau'] == t_]\n",
    "\n",
    "\n",
    "        files = np.unique(df_['file'].values)\n",
    "        rkca = np.zeros(files.size)\n",
    "        rpca = np.zeros(files.size)\n",
    "        for kk, file_ in enumerate(files):\n",
    "            d1 = apply_df_filters(df_, file=file_)\n",
    "            d2 = apply_df_filters(df_2, file=file_)\n",
    "\n",
    "            pdb.set_trace()\n",
    "            rkca[kk] = scipy.stats.spearmanr(d1.iloc[0]['KCA_loading'])\n",
    "\n",
    "\n",
    "\n",
    "        # cor_bins_coef_dca[bin_idx, tidx] = df_['spr_coef_dca'].values\n",
    "        # cor_bins_r2_dca[bin_idx, tidx] = df_['spr_r2_dca'].values\n",
    "        # cor_bins_coef_pca[bin_idx, tidx] = df_['spr_coef_pca'].values\n",
    "        # cor_bins_r2_pca[bin_idx, tidx] = df_['spr_r2_pca'].values\n",
    "        \n",
    "        # cor_bins_dca_tc[bin_idx, tidx] = df_['spr_coef_dca_tc'].values\n",
    "        # cor_bins_dca_tr2[bin_idx, tidx] = df_['spr_r2_dca_tr2'].values\n",
    "\n",
    "        # cor_bins_pca_tc[bin_idx, tidx] = df_['spr_coef_pca_tc'].values\n",
    "        # cor_bins_pca_tr2[bin_idx, tidx] = df_['spr_r2_pca_tr2'].values\n",
    "\n",
    "fig, ax = plt.subplots(5, 4, figsize=(10, 30))\n",
    "\n",
    "for i in range(len(tau)):\n",
    "\n",
    "    a = ax[i, 0]    \n",
    "    a.title.set_text(\"loading&r2 correlation, tau=%f\" % tau[i])\n",
    "    bp1 = box_plot(a, cor_bins_r2_dca[:, i, :], 'black', 'red')\n",
    "    bp2 = box_plot(a, cor_bins_r2_pca[:, i, :], 'black', 'grey')\n",
    "    a.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]], ['DCA', 'PCA'])\n",
    "    \n",
    "    a = ax[i, 1]    \n",
    "    a.title.set_text(\"loading&squared sum of coef correlation, tau=%f\" % tau[i])\n",
    "    bp1 = box_plot(a, cor_bins_coef_dca[:, i, :], 'black', 'red')\n",
    "    bp2 = box_plot(a, cor_bins_coef_pca[:, i, :], 'black', 'grey')\n",
    "    a.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]], ['DCA', 'PCA'])\n",
    "\n",
    "    a = ax[i, 2]    \n",
    "    a.title.set_text(\"Top coef correlation, tau=%f\" % tau[i])\n",
    "    bp1 = box_plot(a, cor_bins_dca_tc[:, i, :], 'black', 'red')\n",
    "    bp2 = box_plot(a, cor_bins_pca_tc[:, i, :], 'black', 'grey')\n",
    "    a.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]], ['DCA', 'PCA'])\n",
    "\n",
    "    a = ax[i, 3]    \n",
    "    a.title.set_text(\"Top r2 correlation, tau=%f\" % tau[i])\n",
    "    bp1 = box_plot(a, cor_bins_dca_tr2[:, i, :], 'black', 'red')\n",
    "    bp2 = box_plot(a, cor_bins_pca_tr2[:, i, :], 'black', 'grey')\n",
    "    a.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]], ['DCA', 'PCA'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
